<!-- TITLE: M201 -->
<!-- SUBTITLE: M201 Performance -->

# Overview
Does not include tuning and sizing.

Requires a local database: `docker run --name mongodb-m201 -p 27017:27017 -d mongo:latest`:
```
mongo
use m201;
exit

mongoimport -d m201 -c people ./people.json
```

# Lessons
## Hardware Considerations
RAM is 25 times fatser SSD, which up to  5 times faster than HDD.

MongoDB works best when everything runs in RAM.

WiredTigfer protocol benefits from a multi-threaded CPU.

MongoDB recommends RAID10 if usning RAID. MongoDB discourage RAID5 and RAID6 (has good redundancy but not performance) and RAID0 (has good performance but no redundancy). MongoDB also recommend using separate disks for OS, logs, indexes and data.

As a distrubted database, MongoDB is dependent on network IO, with primary/secondary cluster nodes, read replicas and shards. The network IO plays heavily into `Write Concern` and `Read Concern` controlled via the application's client connection via the driver.

## Indexes
Indexes improve the response on slow queries. Think of database indexes the same the _index_ in a book, or key/value pairs.

When a query has to look inside the documents on a collection, it is called a **collection scan** (equivalent to a "table scan" in RDBMS). It's a linear operation; the more documents to scan, the longer it takes. A collection scan is an "Order of N" operations; written as **O(N)**.

An index is automatically created on `_id` for every collection.

A MongoDB index is sorted and is stored as a "B-Tree" (https://dzone.com/articles/effective-mongodb-indexing-part-1). 

Indexes are not free. Write performance degrades, because indexes have to be updated when inserting/updating documents, and sometimes it is necessary re-balance the index. Be careful not to have unnecessary indexes (regularly review your indexes).

### Disk Storage
Two main storage engines:
* MMPA V1
* WiredTiger

We have databases which contain collections; collections group documents. Indexes operate over a single collection.

When MongoDB is launched, it is given the data and log paths: `mongod --dbpath /data/db --fork --logpath /data/db/mongodb.log`. Each index is written to a separate file. Each collection is written to a separate file.

When `mongod` starts up, it writes a collectoin files. One being `_mdb_catalog.wt` which references all the files created, including the collections and indexes.

`--directoryperdb` command line option uses separate directory for each MongoDB database; `admin` and `local` are two databases always created to manage the MongoDB instance. With this option, the collection and indexe files are contained in separate directories. And when starting `mongod` with the `--wiredTigerDirectoryForIndexes` collections and indexes are stored in separate directories:

* `/data/db`
	* `/hello`
		* `/collections`
		* `/index`

This allows different disk sets to be mounted thus separate disk IO for collections and indexes; this is good for writes because documents and indexes can be updated concurrently.

MongoDB also used compression on documents; this reduces disk IO but increases CPU usage.

Every DB instance also has a journal file used to recover a database from failure; the `journal` directory contains the journal files. The journal is written to on every write, regardless of the write concern. But the journal can be forced to sync data by specifying the `{j:true}` option on writes:
```
db.collection.insert({}, {
	writeConcer: 1,
	j: true
}
```

### Single Field Indexes
* `db.collection.createIndex({<field>:<direction>})`
* `db.collection.dropIndexes()` - drops all indexes on a collection
* `db.collection.getIndexes()`
* `db.collection.dropIndex("<name of index>)` - drops a given index returned by `getIndexes`

In the example dataase, we will find a single document using the `ssn` field: `db.people.find({"ssn": "720-38-5636"})`.

But to see what MongoDB does when executing that find, issue `db.people.find({"ssn": "720-38-5636"}).explain("executionStats")`:
```
{
	"queryPlanner" : {
		"plannerVersion" : 1,
		"namespace" : "m201.people",
		"indexFilterSet" : false,
		"parsedQuery" : {
			"ssn" : {
				"$eq" : "720-38-5636"
			}
		},
		"winningPlan" : {
			"stage" : "COLLSCAN",
			"filter" : {
				"ssn" : {
					"$eq" : "720-38-5636"
				}
			},
			"direction" : "forward"
		},
		"rejectedPlans" : [ ]
	},
	"executionStats" : {
		"executionSuccess" : true,
		"nReturned" : 1,
		"executionTimeMillis" : 33,
		"totalKeysExamined" : 0,
		"totalDocsExamined" : 50474,
		"executionStages" : {
			"stage" : "COLLSCAN",
			"filter" : {
				"ssn" : {
					"$eq" : "720-38-5636"
				}
			},
			"nReturned" : 1,
			"executionTimeMillisEstimate" : 30,
			"works" : 50476,
			"advanced" : 1,
			"needTime" : 50474,
			"needYield" : 0,
			"saveState" : 394,
			"restoreState" : 394,
			"isEOF" : 1,
			"invalidates" : 0,
			"direction" : "forward",
			"docsExamined" : 50474
		}
	},
	"serverInfo" : {
		"host" : "77613311d01f",
		"port" : 27017,
		"version" : "4.0.9",
		"gitVersion" : "fc525e2d9b0e4bceff5c2201457e564362909765"
	},
	"ok" : 1
}
```

The `winningPlan` says MongoDB is using a collection scan `COLLSCAN` and under `executionStats` highlights the `totalDocsExamined` being 50474 - namely, all documents scanned, to return just one document.

Creating an index is simple: `db.people.createIndex({"ssn":1})` (where 1 says sort order is increasing).

An alternative way to report on execution plans in the mongo shell is to first setup a local variable within the shell on the collection: `myexp = db.people.explain("executionStats")` and then run the query on that local variable: `myexp.find({"ssn": "720-38-5636"})`.:
```
{
	"queryPlanner" : {
		"plannerVersion" : 1,
		"namespace" : "m201.people",
		"indexFilterSet" : false,
		"parsedQuery" : {
			"ssn" : {
				"$eq" : "720-38-5636"
			}
		},
		"winningPlan" : {
			"stage" : "FETCH",
			"inputStage" : {
				"stage" : "IXSCAN",
				"keyPattern" : {
					"ssn" : 1
				},
				"indexName" : "ssn_1",
				"isMultiKey" : false,
				"multiKeyPaths" : {
					"ssn" : [ ]
				},
				"isUnique" : false,
				"isSparse" : false,
				"isPartial" : false,
				"indexVersion" : 2,
				"direction" : "forward",
				"indexBounds" : {
					"ssn" : [
						"[\"720-38-5636\", \"720-38-5636\"]"
					]
				}
			}
		},
		"rejectedPlans" : [ ]
	},
	"executionStats" : {
		"executionSuccess" : true,
		"nReturned" : 1,
		"executionTimeMillis" : 0,
		"totalKeysExamined" : 1,
		"totalDocsExamined" : 1,
		"executionStages" : {
			"stage" : "FETCH",
			"nReturned" : 1,
			"executionTimeMillisEstimate" : 0,
			"works" : 2,
			"advanced" : 1,
			"needTime" : 0,
			"needYield" : 0,
			"saveState" : 0,
			"restoreState" : 0,
			"isEOF" : 1,
			"invalidates" : 0,
			"docsExamined" : 1,
			"alreadyHasObj" : 0,
			"inputStage" : {
				"stage" : "IXSCAN",
				"nReturned" : 1,
				"executionTimeMillisEstimate" : 0,
				"works" : 2,
				"advanced" : 1,
				"needTime" : 0,
				"needYield" : 0,
				"saveState" : 0,
				"restoreState" : 0,
				"isEOF" : 1,
				"invalidates" : 0,
				"keyPattern" : {
					"ssn" : 1
				},
				"indexName" : "ssn_1",
				"isMultiKey" : false,
				"multiKeyPaths" : {
					"ssn" : [ ]
				},
				"isUnique" : false,
				"isSparse" : false,
				"isPartial" : false,
				"indexVersion" : 2,
				"direction" : "forward",
				"indexBounds" : {
					"ssn" : [
						"[\"720-38-5636\", \"720-38-5636\"]"
					]
				},
				"keysExamined" : 1,
				"seeks" : 1,
				"dupsTested" : 0,
				"dupsDropped" : 0,
				"seenInvalidated" : 0
			}
		}
	},
	"serverInfo" : {
		"host" : "77613311d01f",
		"port" : 27017,
		"version" : "4.0.9",
		"gitVersion" : "fc525e2d9b0e4bceff5c2201457e564362909765"
	},
	"ok" : 1
}
```

The `winningPlan` is now a "FETCH" using an index "IXSCAN", and `executionStates` is showing `totalDocsExamined` as 1 and `totalKeysExaminsed` also 1, using the`indexName` "ssn_1".

Indexes can be created on sub documents fields, by using the dot notation. Never index on a sub document itself, because to make use of that index, we'd have to provide the full subdocument in the query.

Indexes can be used not just to find a single document, but also to find a range of documents: `db.people.find({ssn: { $gte: "555-00-0000", $lte: "556-00-0000"}})` - returns 49 documents, with `myexp.find({ssn: { $gte: "555-00-0000", $lte: "556-00-0000"}})` returning:
```{
	"queryPlanner" : {
		"plannerVersion" : 1,
		"namespace" : "m201.people",
		"indexFilterSet" : false,
		"parsedQuery" : {
			"$and" : [
				{
					"ssn" : {
						"$lte" : "556-00-0000"
					}
				},
				{
					"ssn" : {
						"$gte" : "555-00-0000"
					}
				}
			]
		},
		"winningPlan" : {
			"stage" : "FETCH",
			"inputStage" : {
				"stage" : "IXSCAN",
				"keyPattern" : {
					"ssn" : 1
				},
				"indexName" : "ssn_1",
				"isMultiKey" : false,
				"multiKeyPaths" : {
					"ssn" : [ ]
				},
				"isUnique" : false,
				"isSparse" : false,
				"isPartial" : false,
				"indexVersion" : 2,
				"direction" : "forward",
				"indexBounds" : {
					"ssn" : [
						"[\"555-00-0000\", \"556-00-0000\"]"
					]
				}
			}
		},
		"rejectedPlans" : [ ]
	},
	"executionStats" : {
		"executionSuccess" : true,
		"nReturned" : 49,
		"executionTimeMillis" : 0,
		"totalKeysExamined" : 49,
		"totalDocsExamined" : 49,
		"executionStages" : {
			"stage" : "FETCH",
			"nReturned" : 49,
			"executionTimeMillisEstimate" : 0,
			"works" : 50,
			"advanced" : 49,
			"needTime" : 0,
			"needYield" : 0,
			"saveState" : 0,
			"restoreState" : 0,
			"isEOF" : 1,
			"invalidates" : 0,
			"docsExamined" : 49,
			"alreadyHasObj" : 0,
			"inputStage" : {
				"stage" : "IXSCAN",
				"nReturned" : 49,
				"executionTimeMillisEstimate" : 0,
				"works" : 50,
				"advanced" : 49,
				"needTime" : 0,
				"needYield" : 0,
				"saveState" : 0,
				"restoreState" : 0,
				"isEOF" : 1,
				"invalidates" : 0,
				"keyPattern" : {
					"ssn" : 1
				},
				"indexName" : "ssn_1",
				"isMultiKey" : false,
				"multiKeyPaths" : {
					"ssn" : [ ]
				},
				"isUnique" : false,
				"isSparse" : false,
				"isPartial" : false,
				"indexVersion" : 2,
				"direction" : "forward",
				"indexBounds" : {
					"ssn" : [
						"[\"555-00-0000\", \"556-00-0000\"]"
					]
				},
				"keysExamined" : 49,
				"seeks" : 1,
				"dupsTested" : 0,
				"dupsDropped" : 0,
				"seenInvalidated" : 0
			}
		}
	},
	"serverInfo" : {
		"host" : "77613311d01f",
		"port" : 27017,
		"version" : "4.0.9",
		"gitVersion" : "fc525e2d9b0e4bceff5c2201457e564362909765"
	},
	"ok" : 1
}
```

We can see here the `winningPlan` is still an FETCH using IXSCAN, with:
* `"nReturned" : 49`
*	`totalKeysExamined" : 49`
* `"totalDocsExamined" : 49`

Indexes work too on ranges of values too. Consider `myexp.find({ssn: { $in: ["001-29-9184", "177-45-0950", "265-67-9973"]}})`:
```
{
	"queryPlanner" : {
		"plannerVersion" : 1,
		"namespace" : "m201.people",
		"indexFilterSet" : false,
		"parsedQuery" : {
			"ssn" : {
				"$in" : [
					"001-29-9184",
					"177-45-0950",
					"265-67-9973"
				]
			}
		},
		"winningPlan" : {
			"stage" : "FETCH",
			"inputStage" : {
				"stage" : "IXSCAN",
				"keyPattern" : {
					"ssn" : 1
				},
				"indexName" : "ssn_1",
				"isMultiKey" : false,
				"multiKeyPaths" : {
					"ssn" : [ ]
				},
				"isUnique" : false,
				"isSparse" : false,
				"isPartial" : false,
				"indexVersion" : 2,
				"direction" : "forward",
				"indexBounds" : {
					"ssn" : [
						"[\"001-29-9184\", \"001-29-9184\"]",
						"[\"177-45-0950\", \"177-45-0950\"]",
						"[\"265-67-9973\", \"265-67-9973\"]"
					]
				}
			}
		},
		"rejectedPlans" : [ ]
	},
	"executionStats" : {
		"executionSuccess" : true,
		"nReturned" : 3,
		"executionTimeMillis" : 0,
		"totalKeysExamined" : 6,
		"totalDocsExamined" : 3,
		"executionStages" : {
			"stage" : "FETCH",
			"nReturned" : 3,
			"executionTimeMillisEstimate" : 0,
			"works" : 6,
			"advanced" : 3,
			"needTime" : 2,
			"needYield" : 0,
			"saveState" : 0,
			"restoreState" : 0,
			"isEOF" : 1,
			"invalidates" : 0,
			"docsExamined" : 3,
			"alreadyHasObj" : 0,
			"inputStage" : {
				"stage" : "IXSCAN",
				"nReturned" : 3,
				"executionTimeMillisEstimate" : 0,
				"works" : 6,
				"advanced" : 3,
				"needTime" : 2,
				"needYield" : 0,
				"saveState" : 0,
				"restoreState" : 0,
				"isEOF" : 1,
				"invalidates" : 0,
				"keyPattern" : {
					"ssn" : 1
				},
				"indexName" : "ssn_1",
				"isMultiKey" : false,
				"multiKeyPaths" : {
					"ssn" : [ ]
				},
				"isUnique" : false,
				"isSparse" : false,
				"isPartial" : false,
				"indexVersion" : 2,
				"direction" : "forward",
				"indexBounds" : {
					"ssn" : [
						"[\"001-29-9184\", \"001-29-9184\"]",
						"[\"177-45-0950\", \"177-45-0950\"]",
						"[\"265-67-9973\", \"265-67-9973\"]"
					]
				},
				"keysExamined" : 6,
				"seeks" : 3,
				"dupsTested" : 0,
				"dupsDropped" : 0,
				"seenInvalidated" : 0
			}
		}
	},
	"serverInfo" : {
		"host" : "77613311d01f",
		"port" : 27017,
		"version" : "4.0.9",
		"gitVersion" : "fc525e2d9b0e4bceff5c2201457e564362909765"
	},
	"ok" : 1
}
```

We're still using the index, having had to inspect three documents (one for each "ssn") which is efficient, but to get there we had to use six keys.

Indexes are still used even when fileting on multiple fields; MongoDB will use the index first to reduce the document set and then apply the additional filtered fields.

### Sorting
Documents returned by a query can be sorted in memory or by an index. In memory is default. When using an index, the documents will be fetched in the order of the index.

Consider `db.people.find({}, { _id:0, last_name: 1, first_name:1, ssn:1}).sort({ssn:1}).explain("executionStats")` with ascending index on "ssn":
```
{
	"queryPlanner" : {
		"plannerVersion" : 1,
		"namespace" : "m201.people",
		"indexFilterSet" : false,
		"parsedQuery" : {
			
		},
		"winningPlan" : {
			"stage" : "PROJECTION",
			"transformBy" : {
				"_id" : 0,
				"last_name" : 1,
				"first_name" : 1,
				"ssn" : 1
			},
			"inputStage" : {
				"stage" : "FETCH",
				"inputStage" : {
					"stage" : "IXSCAN",
					"keyPattern" : {
						"ssn" : 1
					},
					"indexName" : "ssn_1",
					"isMultiKey" : false,
					"multiKeyPaths" : {
						"ssn" : [ ]
					},
					"isUnique" : false,
					"isSparse" : false,
					"isPartial" : false,
					"indexVersion" : 2,
					"direction" : "forward",
					"indexBounds" : {
						"ssn" : [
							"[MinKey, MaxKey]"
						]
					}
				}
			}
		},
		"rejectedPlans" : [ ]
	},
	"executionStats" : {
		"executionSuccess" : true,
		"nReturned" : 50474,
		"executionTimeMillis" : 59,
		"totalKeysExamined" : 50474,
		"totalDocsExamined" : 50474,
		"executionStages" : {
			"stage" : "PROJECTION",
			"nReturned" : 50474,
			"executionTimeMillisEstimate" : 50,
			"works" : 50475,
			"advanced" : 50474,
			"needTime" : 0,
			"needYield" : 0,
			"saveState" : 394,
			"restoreState" : 394,
			"isEOF" : 1,
			"invalidates" : 0,
			"transformBy" : {
				"_id" : 0,
				"last_name" : 1,
				"first_name" : 1,
				"ssn" : 1
			},
			"inputStage" : {
				"stage" : "FETCH",
				"nReturned" : 50474,
				"executionTimeMillisEstimate" : 30,
				"works" : 50475,
				"advanced" : 50474,
				"needTime" : 0,
				"needYield" : 0,
				"saveState" : 394,
				"restoreState" : 394,
				"isEOF" : 1,
				"invalidates" : 0,
				"docsExamined" : 50474,
				"alreadyHasObj" : 0,
				"inputStage" : {
					"stage" : "IXSCAN",
					"nReturned" : 50474,
					"executionTimeMillisEstimate" : 20,
					"works" : 50475,
					"advanced" : 50474,
					"needTime" : 0,
					"needYield" : 0,
					"saveState" : 394,
					"restoreState" : 394,
					"isEOF" : 1,
					"invalidates" : 0,
					"keyPattern" : {
						"ssn" : 1
					},
					"indexName" : "ssn_1",
					"isMultiKey" : false,
					"multiKeyPaths" : {
						"ssn" : [ ]
					},
					"isUnique" : false,
					"isSparse" : false,
					"isPartial" : false,
					"indexVersion" : 2,
					"direction" : "forward",
					"indexBounds" : {
						"ssn" : [
							"[MinKey, MaxKey]"
						]
					},
					"keysExamined" : 50474,
					"seeks" : 1,
					"dupsTested" : 0,
					"dupsDropped" : 0,
					"seenInvalidated" : 0
				}
			}
		}
	},
	"serverInfo" : {
		"host" : "77613311d01f",
		"port" : 27017,
		"version" : "4.0.9",
		"gitVersion" : "fc525e2d9b0e4bceff5c2201457e564362909765"
	},
	"ok" : 1
}
```

The `winningPlan` is showing a FETCH using IXSCAN. Here, `nReturned` is 50474 (all documents) with 50474 keys used and and 50474 documents examined; this is because we didn't limit the number of documents.

The index will still be used even if the sort order is reversed, viz: `db.people.find({}, { _id:0, last_name: 1, first_name:1, ssn:1}).sort({ssn:-1}).explain("executionStats")`; the `"direction" : "backward"`.

## Compound Indexes
Indexes are compounded not multi-dimensional. Even though a index has multiple fields (each of which can have a separate sort order), each index key is concatenated with each field.

The sequence of the fields in a compound index is important. Consider a compound index on both `last_name` and `first_name`:
* if the query searches on both, it will get fast results
* if the query searches on just `last_name` it will get a fast set of all keys with the `last_name`.
* but if query searches on just `first_name`, then the index won't be used at all.

Compound indexes are good when expecting to work with all fields on the compound or work with a set of ordered results on some of the fields.

### Index Prefixes
https://docs.mongodb.com/manual/core/index-compound/#compound-index-prefix

The MongoDB query optimiser will choose the best index if there are multiple indexes on the same fields being filtered upon, and more so, will only choose compound indexes when filtering/sorting on fields in the left to right order of the index. In the above compound example of `first_name` and `last_name`, a compound index can be used on `last_name` then `first_name` to satisfy the first two queries and a separate single field index on `first_name` alone for the rarer occasions when filtering on `first_name` alone.

Alternatively, rather than creating multiple indexes, considering writing the queries to utilise the compound index. But note, sometimes it is fatser to do a collection scan than it is to work through all keys in an index.

Index prefixes still apply when sorting on fields within the index compound. The combination of query filter and sort must also match on a compound filter prefix sequence.

Significantly for sorting with compound indexes, the sort order for each comfound field must be consistent against the sort order of the fields within the index.


## Covered Queries
A covered queries is where all the match/project/sort fields are present in the query, thus MongoDB is able to return data from the index alone, without fetching any documents.

## Multikey Indexes
Indexing on a field that is an array. MondoDB will create separate keys for each value in the array. Can index on primative array values, but we can also index on fields within an array of documents. Consider:
```
{
	_id: ObjectId(""),
	productName: "MongoDB Long Sleave T-Shirt",
	stock: [
		{ size: "S", color:"red", quantity:25 },
		{ size: "M", color:"red", quantity:15 },
		{ size: "L", color:"red", quantity:5 }
	]
}
```

We can create an index on "color": `db.catalog.createIndex({stock.color: 1})`.

When creating a compound index, up to just one field can be an array.

The number of items within the array must be kept low, to prevent an explosion on number of keys. Be this is mind when attempting to applying the [attribute](https://wozitech.asuscomm.com/mongodb/uni/m320#attribute-pattern) pattern.

More keys in your indexes, the more memory required to use the index.

Multikey indexes do not support covered queires.

## Partial Indexes
There are times we only want to index some of the documents within a collection; this is known as a partial index. This is done by giving a condition upon creating the index. For example:
```
db.collection.createIndex(
	{ "address.city":1, cuisine: 1 },
	{ partialFilterExpression: { stars: { $gte: 3.5 } } }
);
```

An index will only exist for those documents where the `stars` field is at least 3.5.

Partial indexes are good for multiley indexes, in that an index can be create for some of the array values rather than all, recducing the effective index size.

For partial indexes to be used, the query predicate/filter must include the condition used in creating the partial index. For the example above, it must include `stars` in the filter and that the `stars` filter value is at least 3.5:
* uses partial index - `db.collection.find({"address.city":"London", "cuisine":"Indian", stars: { $gt: 4 }}`
* fails to use partial index - `db.collection.find({"address.city":"London", "cuisine":"Indian", stars: { $lt: 3 }}`
* fails to use partial index -  `db.collection.find({"address.city":"London", "cuisine":"Indian"}`

Cannot create a partial index on `_id` (all documents must have `_id` indexed). Shard Keys cannot be partial indexes (all documents must belong to one shard and must be able to locate the shard quickly through the indexed shared key).

## Sparse Indexes
A sparse index is where a document must have a value for the given field:
```
db.collection.createIndex(
	{ "colour":1 },
	{ sparse: true }
);
```

This is equivalent to a partial index of:
```
db.collection.createIndex(
	{ "colour":1 },
	{ partialFilterExpression: { colour: { $exists: true } } }
);
```

The default is not-sparse, with a document not having the field treated as a null index value.

## Text Indexes
https://docs.mongodb.com/manual/text-search/

```
db.collection.createIndex(
	{ "colour": "text" },
	{ sparse: true }
);
```

A single text index can be created on a collection, covering multiple fields within the document to search text on. MongoDB creates an index key for each word in all given fields, and includes hyphens to be a single word. All the keys are lowercase. This makes some assumptions that the key is a latin (english) style word.

Text indexes can get very large very quickly. Need to review frequently to ensure that the index will fit in memory. A useful skill here is to use a compound text indexes, so prefixing the text index with a value field, e.g.:

```
db.collection.createIndex(
	{ "size": 1, "colour": "text" },
	{ sparse: true }
);
```

Queries have to be written to explicitly utilise text indexes: `db.collection.find({ $text: { $search: "red"} })`. `$text` queries searches on each of the words given; so will return documents even having just one of the search words. Can incldue the ranking of the results, through prrojecting the "textScore": `db.collection.find({ $text: { $search: "red"} }, { score: { $meta: "textScore" } }).sort({score:-1})`.

### Collations
Is the language constructs used to create and sort indexes and includes:
* `locale` - this is the langauge local, e.g. "en-US" or "en-GB".
* `caseLevel`
* `caseFirst`
* `strength`
* `numericOrdering`
* `alternate`
* `maxVariable`
* `backwards`

A collation can be defined for a specific collection; must create the collection first using `db.createCollection("<name>", { collation: { locale: "en-GB" }})`.

Collation can be used when fetching results, e.g. on cursor returned from find.

Collection can be specified when creating an index. **When running queries/aggregation against collections with indexes having specific collations, then the query/aggregation must specify the same collation for the index to be used.**

The most frequent use for a collation is for case insensitive index keys, or index keys that better support numbers, e.g. "1hsj, 2hsj, 3hsj, ...., 9hsj, 10hsj", rather than "1hsj, 10hsj, 2hsj, ...".

## Index Operations
The tradeoff between background and foregound indexes, using `currentOp` and `killOp`.

In production MongoDB environment, you want to minimise downtime/impact. Foreground indexes, although faster to be built, block any access database (not just the collection) until completed. Background indexes do not block. Background indexes still impacts on data load.performance, but has less impact.

### Background Indexes
Simply pass the option `background` of `true` when creating the index:
```
db.collection.createIndex(
	{ "size": 1, "colour": "text" },
	{ background: true }
);
```

Even though the index is built in the background, the `mongo` shell command will not return until the index has been built.

### currentOp
The `db.currentOp` command can be issued via `mongo` shell to inspect running activities - as shown below when there is nothing else running:
```
> db.currentOp()
{
	"inprog" : [
		{
			"host" : "77613311d01f:27017",
			"desc" : "conn13",
			"connectionId" : 13,
			"client" : "172.17.0.1:56900",
			"appName" : "MongoDB Shell",
			"clientMetadata" : {
				"application" : {
					"name" : "MongoDB Shell"
				},
				"driver" : {
					"name" : "MongoDB Internal Client",
					"version" : "4.0.3"
				},
				"os" : {
					"type" : "Linux",
					"name" : "Fedora release 29 (Twenty Nine)",
					"architecture" : "x86_64",
					"version" : "Kernel 5.2.18-100.fc29.x86_64"
				}
			},
			"active" : true,
			"currentOpTime" : "2019-11-05T14:14:29.684+0000",
			"opid" : 310781,
			"lsid" : {
				"id" : UUID("e3eb5a4d-a296-4266-a990-cf5178c7c4e0"),
				"uid" : BinData(0,"47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU=")
			},
			"secs_running" : NumberLong(0),
			"microsecs_running" : NumberLong(52),
			"op" : "command",
			"ns" : "admin.$cmd.aggregate",
			"command" : {
				"currentOp" : 1,
				"lsid" : {
					"id" : UUID("e3eb5a4d-a296-4266-a990-cf5178c7c4e0")
				},
				"$db" : "admin"
			},
			"numYields" : 0,
			"locks" : {
				
			},
			"waitingForLock" : false,
			"lockStats" : {
				
			}
		}
	],
	"ok" : 1
}
```

`currentOp` can also filter on running tasks (using the `db.find` filter syntax):
```
db.currentOp({
	$or: [
		{ op: "command", "query.createIndexes" : { $exist: true} },
		{ op: "insert", ns: /\n.system\.indexes\b/ }
	]
})
```

Every operation as a unique `opId` as returned in `currentOp`. Can then use `db.killOp(<opId>)` to forcibly stop long running processes.


## Query Plans
When a query arrives as the mongodb primary for the first time, the query optimiser
1. Looks at all **available** indexes and identifies the set of **candidate** indexes that could be used.
2. As a **imperical optimiser**, mongod will execute the query against all candidate indexes, but for a short time, to gauge the effectiveness of each.
3. Chooses the "best" plan - "best" is not a simpler thing to rationalise - it could be the fastest, the less memory/disk IO, ...

MongoDB caches the plans for a given query which includes stats about how it got to decide upon the winning plan, and is able to then execute the query with the "best" index previously determined.

But indexes and data change over time, so mongod updates the query cache whenever:
* the database is restarted
* whenever the stats over a short time to execute the planned query exceeds a threshold as recorded when "caching the stats of the best plan"
* when indexes are rebuilt
* when index are created/dropped.

## explain
* `var exp = db.<collection.explain()` - reports on each stage of the query execution without executing the query
* `var exp = db.<collection.explain(queryPlanner)` - the same as passing no argument (default)
* `var exp = db.<collection.explain("executionStats)` - as per default report but executes the query  to provide additional infomation on execution resources (such as, number of documents fetched, indexes used, execution time, ....)
* `var exp = db.<collection.explain("allPlansExecuted)` - this is the most verbose mode.

When reading the stages, they need to be read in reverse order. Given the following:
```
{
	"queryPlanner" : {
		"plannerVersion" : 1,
		"namespace" : "m201.people",
		"indexFilterSet" : false,
		"parsedQuery" : {
			
		},
		"winningPlan" : {
			"stage" : "PROJECTION",
			"transformBy" : {
				"_id" : 0,
				"last_name" : 1,
				"first_name" : 1,
				"ssn" : 1
			},
			"inputStage" : {
				"stage" : "FETCH",
				"inputStage" : {
					"stage" : "IXSCAN",
					"keyPattern" : {
						"ssn" : 1
					},
					"indexName" : "ssn_1",
					"isMultiKey" : false,
					"multiKeyPaths" : {
						"ssn" : [ ]
					},
					"isUnique" : false,
					"isSparse" : false,
					"isPartial" : false,
					"indexVersion" : 2,
					"direction" : "forward",
					"indexBounds" : {
						"ssn" : [
							"[MinKey, MaxKey]"
						]
					}
				}
			}
		},
		"rejectedPlans" : [ ]
}
```
	
The stages are:
* IXSCAN
* FETCH
* PROJECT

Within the `executionStats` results, we ideally want the `totalKeysExamined` and `totalDocsExamined` to be as close to the `nReturned` (number of documents).

If there is just one contending index against the query on the collection, then the `explain` output will only provide the `winningPlan`. With more than one contending index though, `explain` output will include `winningPlan` and `rejectedPlans` in output.

When a sort stage is involved, `explain` results will include `memUsage` (the memory used to complete the sort) along with `memLimit` the amount of available memory. This is especially useful given sorts are bound by RAM.

### Shards
When a collection is spread out over shards, each shard is responsible for optimising queries sent to it to execute, and thus each shard could have a different winning plan (all depends on how data is spread across the shards). `explain` results the winning/rejected plans for each shard.

## Hints
The query optimiseer is not perfect, and there may be times wanting to override mongod query optimiser by giving hints in query, by supplementing the `find` with a `hint`:
```
db.people.find({
    "address.state": "Nebraska",
    "last_name": /^G/,
    "job": "Police officer"
  }).hint({"address.state":1, "job":1})
```

Where the document passed to the hint is the document used when creating the index.

With aggregation, it is given as an option, knowing the name of the index (as return be `db.<collection>.getIndexes()`:
```
db.people.aggregate([
	{
		$match : {
			"address.state": "Nebraska",
			"last_name": /^G/,
			"job": "Police officer"
		}
	}
], {
	hint: "index_1"
})
```

Note - the most possible reason why the query optimiser fails is owing to multiple indexes in contention; better to review and reduce those indexes.


## Resource Allocations
Each indexes consumes storage and memory, therefore it is necesaary to inspect what resources are being used.

Using Compass, it is easy to see the size of collections and the total size of all indexes per collection against the database view. On the collection view, indexes tab, it is easy to see the size of each index on the collection and the index usage (number of times used since created).

Via the `mongo` shell, `db.stats` returns the size of the database:
```
> db.stats()
{
	"db" : "m201",
	"collections" : 1,
	"views" : 0,
	"objects" : 50474,
	"avgObjSize" : 387.80518682886236,
	"dataSize" : 19574079,
	"storageSize" : 11403264,
	"numExtents" : 0,
	"indexes" : 3,
	"indexSize" : 1740800,
	"fsUsedSize" : 24712011776,
	"fsTotalSize" : 107363696640,
	"ok" : 1
}
```

`db.<collection>.stats` returns information on the collection itself (a lot of inforamtion as seen below), but includes `nindexes`, `totalIndexSize` and `indexSizes`:
```
{
	"ns" : "m201.people",
	"size" : 19574079,
	"count" : 50474,
	"avgObjSize" : 387,
	"storageSize" : 11403264,
	"capped" : false,
	"wiredTiger" : {
		"metadata" : {
			"formatVersion" : 1
		},
		"creationString" : "access_pattern_hint=none,allocation_size=4KB,app_metadata=(formatVersion=1),assert=(commit_timestamp=none,read_timestamp=none),block_allocation=best,block_compressor=snappy,cache_resident=false,checksum=on,colgroups=,collator=,columns=,dictionary=0,encryption=(keyid=,name=),exclusive=false,extractor=,format=btree,huffman_key=,huffman_value=,ignore_in_memory_cache_size=false,immutable=false,internal_item_max=0,internal_key_max=0,internal_key_truncate=true,internal_page_max=4KB,key_format=q,key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=32KB,leaf_value_max=64MB,log=(enabled=true),lsm=(auto_throttle=true,bloom=true,bloom_bit_count=16,bloom_config=,bloom_hash_count=8,bloom_oldest=false,chunk_count_limit=0,chunk_max=5GB,chunk_size=10MB,merge_custom=(prefix=,start_generation=0,suffix=),merge_max=15,merge_min=0),memory_page_image_max=0,memory_page_max=10m,os_cache_dirty_max=0,os_cache_max=0,prefix_compression=false,prefix_compression_min=4,source=,split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,type=file,value_format=u",
		"type" : "file",
		"uri" : "statistics:table:collection-7-3181334468023989821",
		"LSM" : {
			"bloom filter false positives" : 0,
			"bloom filter hits" : 0,
			"bloom filter misses" : 0,
			"bloom filter pages evicted from cache" : 0,
			"bloom filter pages read into cache" : 0,
			"bloom filters in the LSM tree" : 0,
			"chunks in the LSM tree" : 0,
			"highest merge generation in the LSM tree" : 0,
			"queries that could have benefited from a Bloom filter that did not exist" : 0,
			"sleep for LSM checkpoint throttle" : 0,
			"sleep for LSM merge throttle" : 0,
			"total size of bloom filters" : 0
		},
		"block-manager" : {
			"allocations requiring file extension" : 707,
			"blocks allocated" : 707,
			"blocks freed" : 0,
			"checkpoint size" : 11390976,
			"file allocation unit size" : 4096,
			"file bytes available for reuse" : 0,
			"file magic number" : 120897,
			"file major version number" : 1,
			"file size in bytes" : 11403264,
			"minor version number" : 0
		},
		"btree" : {
			"btree checkpoint generation" : 3344,
			"column-store fixed-size leaf pages" : 0,
			"column-store internal pages" : 0,
			"column-store variable-size RLE encoded values" : 0,
			"column-store variable-size deleted values" : 0,
			"column-store variable-size leaf pages" : 0,
			"fixed-record size" : 0,
			"maximum internal page key size" : 368,
			"maximum internal page size" : 4096,
			"maximum leaf page key size" : 2867,
			"maximum leaf page size" : 32768,
			"maximum leaf page value size" : 67108864,
			"maximum tree depth" : 3,
			"number of key/value pairs" : 0,
			"overflow pages" : 0,
			"pages rewritten by compaction" : 0,
			"row-store internal pages" : 0,
			"row-store leaf pages" : 0
		},
		"cache" : {
			"bytes currently in the cache" : 25229377,
			"bytes dirty in the cache cumulative" : 1996,
			"bytes read into cache" : 0,
			"bytes written from cache" : 19956791,
			"checkpoint blocked page eviction" : 0,
			"data source pages selected for eviction unable to be evicted" : 0,
			"eviction walk passes of a file" : 0,
			"eviction walk target pages histogram - 0-9" : 0,
			"eviction walk target pages histogram - 10-31" : 0,
			"eviction walk target pages histogram - 128 and higher" : 0,
			"eviction walk target pages histogram - 32-63" : 0,
			"eviction walk target pages histogram - 64-128" : 0,
			"eviction walks abandoned" : 0,
			"eviction walks gave up because they restarted their walk twice" : 0,
			"eviction walks gave up because they saw too many pages and found no candidates" : 0,
			"eviction walks gave up because they saw too many pages and found too few candidates" : 0,
			"eviction walks reached end of tree" : 0,
			"eviction walks started from root of tree" : 0,
			"eviction walks started from saved location in tree" : 0,
			"hazard pointer blocked page eviction" : 0,
			"in-memory page passed criteria to be split" : 4,
			"in-memory page splits" : 2,
			"internal pages evicted" : 0,
			"internal pages split during eviction" : 0,
			"leaf pages split during eviction" : 0,
			"modified pages evicted" : 0,
			"overflow pages read into cache" : 0,
			"page split during eviction deepened the tree" : 0,
			"page written requiring cache overflow records" : 0,
			"pages read into cache" : 0,
			"pages read into cache after truncate" : 1,
			"pages read into cache after truncate in prepare state" : 0,
			"pages read into cache requiring cache overflow entries" : 0,
			"pages requested from the cache" : 350138,
			"pages seen by eviction walk" : 0,
			"pages written from cache" : 706,
			"pages written requiring in-memory restoration" : 0,
			"tracked dirty bytes in the cache" : 0,
			"unmodified pages evicted" : 0
		},
		"cache_walk" : {
			"Average difference between current eviction generation when the page was last considered" : 0,
			"Average on-disk page image size seen" : 0,
			"Average time in cache for pages that have been visited by the eviction server" : 0,
			"Average time in cache for pages that have not been visited by the eviction server" : 0,
			"Clean pages currently in cache" : 0,
			"Current eviction generation" : 0,
			"Dirty pages currently in cache" : 0,
			"Entries in the root page" : 0,
			"Internal pages currently in cache" : 0,
			"Leaf pages currently in cache" : 0,
			"Maximum difference between current eviction generation when the page was last considered" : 0,
			"Maximum page size seen" : 0,
			"Minimum on-disk page image size seen" : 0,
			"Number of pages never visited by eviction server" : 0,
			"On-disk page image sizes smaller than a single allocation unit" : 0,
			"Pages created in memory and never written" : 0,
			"Pages currently queued for eviction" : 0,
			"Pages that could not be queued for eviction" : 0,
			"Refs skipped during cache traversal" : 0,
			"Size of the root page" : 0,
			"Total number of pages currently in cache" : 0
		},
		"compression" : {
			"compressed pages read" : 0,
			"compressed pages written" : 702,
			"page written failed to compress" : 0,
			"page written was too small to compress" : 4
		},
		"cursor" : {
			"bulk-loaded cursor-insert calls" : 0,
			"close calls that result in cache" : 0,
			"create calls" : 12,
			"cursor operation restarted" : 0,
			"cursor-insert key and value bytes inserted" : 19716927,
			"cursor-remove key bytes removed" : 0,
			"cursor-update value bytes updated" : 0,
			"cursors reused from cache" : 152,
			"insert calls" : 50474,
			"modify calls" : 0,
			"next calls" : 1504611,
			"open cursor count" : 0,
			"prev calls" : 1,
			"remove calls" : 0,
			"reserve calls" : 0,
			"reset calls" : 19139,
			"search calls" : 287033,
			"search near calls" : 11914,
			"truncate calls" : 0,
			"update calls" : 0
		},
		"reconciliation" : {
			"dictionary matches" : 0,
			"fast-path pages deleted" : 0,
			"internal page key bytes discarded using suffix compression" : 1405,
			"internal page multi-block writes" : 1,
			"internal-page overflow keys" : 0,
			"leaf page key bytes discarded using prefix compression" : 0,
			"leaf page multi-block writes" : 3,
			"leaf-page overflow keys" : 0,
			"maximum blocks required for a page" : 1,
			"overflow values written" : 0,
			"page checksum matches" : 0,
			"page reconciliation calls" : 5,
			"page reconciliation calls for eviction" : 0,
			"pages deleted" : 0
		},
		"session" : {
			"object compaction" : 0
		},
		"transaction" : {
			"update conflicts" : 0
		}
	},
	"nindexes" : 3,
	"totalIndexSize" : 1740800,
	"indexSizes" : {
		"_id_" : 462848,
		"job_1_address.state_1_first_name_1" : 966656,
		"job_1" : 311296
	},
	"ok" : 1
}
```

### Disk and Memory
Indexes consume both disk and memory. Typically, disk contention is not an issue for indexes. If there is no disk space for an index, you have bigger issues.

Indexes however should be held fully within RAM. To inspect how much of an index is held in memory, can execute the collection's stat command passing `indexDetails:true`:
```
{
	"ns" : "m201.people",
	"size" : 19574079,
	"count" : 50474,
	"avgObjSize" : 387,
	"storageSize" : 11403264,
	"capped" : false,
	"wiredTiger" : {
		"metadata" : {
			"formatVersion" : 1
		},
		"creationString" : "access_pattern_hint=none,allocation_size=4KB,app_metadata=(formatVersion=1),assert=(commit_timestamp=none,read_timestamp=none),block_allocation=best,block_compressor=snappy,cache_resident=false,checksum=on,colgroups=,collator=,columns=,dictionary=0,encryption=(keyid=,name=),exclusive=false,extractor=,format=btree,huffman_key=,huffman_value=,ignore_in_memory_cache_size=false,immutable=false,internal_item_max=0,internal_key_max=0,internal_key_truncate=true,internal_page_max=4KB,key_format=q,key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=32KB,leaf_value_max=64MB,log=(enabled=true),lsm=(auto_throttle=true,bloom=true,bloom_bit_count=16,bloom_config=,bloom_hash_count=8,bloom_oldest=false,chunk_count_limit=0,chunk_max=5GB,chunk_size=10MB,merge_custom=(prefix=,start_generation=0,suffix=),merge_max=15,merge_min=0),memory_page_image_max=0,memory_page_max=10m,os_cache_dirty_max=0,os_cache_max=0,prefix_compression=false,prefix_compression_min=4,source=,split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,type=file,value_format=u",
		"type" : "file",
		"uri" : "statistics:table:collection-7-3181334468023989821",
		"LSM" : {
			"bloom filter false positives" : 0,
			"bloom filter hits" : 0,
			"bloom filter misses" : 0,
			"bloom filter pages evicted from cache" : 0,
			"bloom filter pages read into cache" : 0,
			"bloom filters in the LSM tree" : 0,
			"chunks in the LSM tree" : 0,
			"highest merge generation in the LSM tree" : 0,
			"queries that could have benefited from a Bloom filter that did not exist" : 0,
			"sleep for LSM checkpoint throttle" : 0,
			"sleep for LSM merge throttle" : 0,
			"total size of bloom filters" : 0
		},
		"block-manager" : {
			"allocations requiring file extension" : 707,
			"blocks allocated" : 707,
			"blocks freed" : 0,
			"checkpoint size" : 11390976,
			"file allocation unit size" : 4096,
			"file bytes available for reuse" : 0,
			"file magic number" : 120897,
			"file major version number" : 1,
			"file size in bytes" : 11403264,
			"minor version number" : 0
		},
		"btree" : {
			"btree checkpoint generation" : 3361,
			"column-store fixed-size leaf pages" : 0,
			"column-store internal pages" : 0,
			"column-store variable-size RLE encoded values" : 0,
			"column-store variable-size deleted values" : 0,
			"column-store variable-size leaf pages" : 0,
			"fixed-record size" : 0,
			"maximum internal page key size" : 368,
			"maximum internal page size" : 4096,
			"maximum leaf page key size" : 2867,
			"maximum leaf page size" : 32768,
			"maximum leaf page value size" : 67108864,
			"maximum tree depth" : 3,
			"number of key/value pairs" : 0,
			"overflow pages" : 0,
			"pages rewritten by compaction" : 0,
			"row-store internal pages" : 0,
			"row-store leaf pages" : 0
		},
		"cache" : {
			"bytes currently in the cache" : 25229377,
			"bytes dirty in the cache cumulative" : 1996,
			"bytes read into cache" : 0,
			"bytes written from cache" : 19956791,
			"checkpoint blocked page eviction" : 0,
			"data source pages selected for eviction unable to be evicted" : 0,
			"eviction walk passes of a file" : 0,
			"eviction walk target pages histogram - 0-9" : 0,
			"eviction walk target pages histogram - 10-31" : 0,
			"eviction walk target pages histogram - 128 and higher" : 0,
			"eviction walk target pages histogram - 32-63" : 0,
			"eviction walk target pages histogram - 64-128" : 0,
			"eviction walks abandoned" : 0,
			"eviction walks gave up because they restarted their walk twice" : 0,
			"eviction walks gave up because they saw too many pages and found no candidates" : 0,
			"eviction walks gave up because they saw too many pages and found too few candidates" : 0,
			"eviction walks reached end of tree" : 0,
			"eviction walks started from root of tree" : 0,
			"eviction walks started from saved location in tree" : 0,
			"hazard pointer blocked page eviction" : 0,
			"in-memory page passed criteria to be split" : 4,
			"in-memory page splits" : 2,
			"internal pages evicted" : 0,
			"internal pages split during eviction" : 0,
			"leaf pages split during eviction" : 0,
			"modified pages evicted" : 0,
			"overflow pages read into cache" : 0,
			"page split during eviction deepened the tree" : 0,
			"page written requiring cache overflow records" : 0,
			"pages read into cache" : 0,
			"pages read into cache after truncate" : 1,
			"pages read into cache after truncate in prepare state" : 0,
			"pages read into cache requiring cache overflow entries" : 0,
			"pages requested from the cache" : 350138,
			"pages seen by eviction walk" : 0,
			"pages written from cache" : 706,
			"pages written requiring in-memory restoration" : 0,
			"tracked dirty bytes in the cache" : 0,
			"unmodified pages evicted" : 0
		},
		"cache_walk" : {
			"Average difference between current eviction generation when the page was last considered" : 0,
			"Average on-disk page image size seen" : 0,
			"Average time in cache for pages that have been visited by the eviction server" : 0,
			"Average time in cache for pages that have not been visited by the eviction server" : 0,
			"Clean pages currently in cache" : 0,
			"Current eviction generation" : 0,
			"Dirty pages currently in cache" : 0,
			"Entries in the root page" : 0,
			"Internal pages currently in cache" : 0,
			"Leaf pages currently in cache" : 0,
			"Maximum difference between current eviction generation when the page was last considered" : 0,
			"Maximum page size seen" : 0,
			"Minimum on-disk page image size seen" : 0,
			"Number of pages never visited by eviction server" : 0,
			"On-disk page image sizes smaller than a single allocation unit" : 0,
			"Pages created in memory and never written" : 0,
			"Pages currently queued for eviction" : 0,
			"Pages that could not be queued for eviction" : 0,
			"Refs skipped during cache traversal" : 0,
			"Size of the root page" : 0,
			"Total number of pages currently in cache" : 0
		},
		"compression" : {
			"compressed pages read" : 0,
			"compressed pages written" : 702,
			"page written failed to compress" : 0,
			"page written was too small to compress" : 4
		},
		"cursor" : {
			"bulk-loaded cursor-insert calls" : 0,
			"close calls that result in cache" : 0,
			"create calls" : 12,
			"cursor operation restarted" : 0,
			"cursor-insert key and value bytes inserted" : 19716927,
			"cursor-remove key bytes removed" : 0,
			"cursor-update value bytes updated" : 0,
			"cursors reused from cache" : 152,
			"insert calls" : 50474,
			"modify calls" : 0,
			"next calls" : 1504611,
			"open cursor count" : 0,
			"prev calls" : 1,
			"remove calls" : 0,
			"reserve calls" : 0,
			"reset calls" : 19139,
			"search calls" : 287033,
			"search near calls" : 11914,
			"truncate calls" : 0,
			"update calls" : 0
		},
		"reconciliation" : {
			"dictionary matches" : 0,
			"fast-path pages deleted" : 0,
			"internal page key bytes discarded using suffix compression" : 1405,
			"internal page multi-block writes" : 1,
			"internal-page overflow keys" : 0,
			"leaf page key bytes discarded using prefix compression" : 0,
			"leaf page multi-block writes" : 3,
			"leaf-page overflow keys" : 0,
			"maximum blocks required for a page" : 1,
			"overflow values written" : 0,
			"page checksum matches" : 0,
			"page reconciliation calls" : 5,
			"page reconciliation calls for eviction" : 0,
			"pages deleted" : 0
		},
		"session" : {
			"object compaction" : 0
		},
		"transaction" : {
			"update conflicts" : 0
		}
	},
	"nindexes" : 3,
	"indexDetails" : {
		"_id_" : {
			"metadata" : {
				"formatVersion" : 8,
				"infoObj" : "{ \"v\" : 2, \"key\" : { \"_id\" : 1 }, \"name\" : \"_id_\", \"ns\" : \"m201.people\" }"
			},
			"creationString" : "access_pattern_hint=none,allocation_size=4KB,app_metadata=(formatVersion=8,infoObj={ \"v\" : 2, \"key\" : { \"_id\" : 1 }, \"name\" : \"_id_\", \"ns\" : \"m201.people\" }),assert=(commit_timestamp=none,read_timestamp=none),block_allocation=best,block_compressor=,cache_resident=false,checksum=on,colgroups=,collator=,columns=,dictionary=0,encryption=(keyid=,name=),exclusive=false,extractor=,format=btree,huffman_key=,huffman_value=,ignore_in_memory_cache_size=false,immutable=false,internal_item_max=0,internal_key_max=0,internal_key_truncate=true,internal_page_max=16k,key_format=u,key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=16k,leaf_value_max=0,log=(enabled=true),lsm=(auto_throttle=true,bloom=true,bloom_bit_count=16,bloom_config=,bloom_hash_count=8,bloom_oldest=false,chunk_count_limit=0,chunk_max=5GB,chunk_size=10MB,merge_custom=(prefix=,start_generation=0,suffix=),merge_max=15,merge_min=0),memory_page_image_max=0,memory_page_max=5MB,os_cache_dirty_max=0,os_cache_max=0,prefix_compression=true,prefix_compression_min=4,source=,split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,type=file,value_format=u",
			"type" : "file",
			"uri" : "statistics:table:index-8-3181334468023989821",
			"LSM" : {
				"bloom filter false positives" : 0,
				"bloom filter hits" : 0,
				"bloom filter misses" : 0,
				"bloom filter pages evicted from cache" : 0,
				"bloom filter pages read into cache" : 0,
				"bloom filters in the LSM tree" : 0,
				"chunks in the LSM tree" : 0,
				"highest merge generation in the LSM tree" : 0,
				"queries that could have benefited from a Bloom filter that did not exist" : 0,
				"sleep for LSM checkpoint throttle" : 0,
				"sleep for LSM merge throttle" : 0,
				"total size of bloom filters" : 0
			},
			"block-manager" : {
				"allocations requiring file extension" : 30,
				"blocks allocated" : 30,
				"blocks freed" : 0,
				"checkpoint size" : 450560,
				"file allocation unit size" : 4096,
				"file bytes available for reuse" : 0,
				"file magic number" : 120897,
				"file major version number" : 1,
				"file size in bytes" : 462848,
				"minor version number" : 0
			},
			"btree" : {
				"btree checkpoint generation" : 3361,
				"column-store fixed-size leaf pages" : 0,
				"column-store internal pages" : 0,
				"column-store variable-size RLE encoded values" : 0,
				"column-store variable-size deleted values" : 0,
				"column-store variable-size leaf pages" : 0,
				"fixed-record size" : 0,
				"maximum internal page key size" : 1474,
				"maximum internal page size" : 16384,
				"maximum leaf page key size" : 1474,
				"maximum leaf page size" : 16384,
				"maximum leaf page value size" : 7372,
				"maximum tree depth" : 3,
				"number of key/value pairs" : 0,
				"overflow pages" : 0,
				"pages rewritten by compaction" : 0,
				"row-store internal pages" : 0,
				"row-store leaf pages" : 0
			},
			"cache" : {
				"bytes currently in the cache" : 5671296,
				"bytes dirty in the cache cumulative" : 1210,
				"bytes read into cache" : 0,
				"bytes written from cache" : 406233,
				"checkpoint blocked page eviction" : 0,
				"data source pages selected for eviction unable to be evicted" : 0,
				"eviction walk passes of a file" : 0,
				"eviction walk target pages histogram - 0-9" : 0,
				"eviction walk target pages histogram - 10-31" : 0,
				"eviction walk target pages histogram - 128 and higher" : 0,
				"eviction walk target pages histogram - 32-63" : 0,
				"eviction walk target pages histogram - 64-128" : 0,
				"eviction walks abandoned" : 0,
				"eviction walks gave up because they restarted their walk twice" : 0,
				"eviction walks gave up because they saw too many pages and found no candidates" : 0,
				"eviction walks gave up because they saw too many pages and found too few candidates" : 0,
				"eviction walks reached end of tree" : 0,
				"eviction walks started from root of tree" : 0,
				"eviction walks started from saved location in tree" : 0,
				"hazard pointer blocked page eviction" : 0,
				"in-memory page passed criteria to be split" : 2,
				"in-memory page splits" : 1,
				"internal pages evicted" : 0,
				"internal pages split during eviction" : 0,
				"leaf pages split during eviction" : 0,
				"modified pages evicted" : 0,
				"overflow pages read into cache" : 0,
				"page split during eviction deepened the tree" : 0,
				"page written requiring cache overflow records" : 0,
				"pages read into cache" : 0,
				"pages read into cache after truncate" : 1,
				"pages read into cache after truncate in prepare state" : 0,
				"pages read into cache requiring cache overflow entries" : 0,
				"pages requested from the cache" : 50475,
				"pages seen by eviction walk" : 0,
				"pages written from cache" : 29,
				"pages written requiring in-memory restoration" : 0,
				"tracked dirty bytes in the cache" : 0,
				"unmodified pages evicted" : 0
			},
			"cache_walk" : {
				"Average difference between current eviction generation when the page was last considered" : 0,
				"Average on-disk page image size seen" : 0,
				"Average time in cache for pages that have been visited by the eviction server" : 0,
				"Average time in cache for pages that have not been visited by the eviction server" : 0,
				"Clean pages currently in cache" : 0,
				"Current eviction generation" : 0,
				"Dirty pages currently in cache" : 0,
				"Entries in the root page" : 0,
				"Internal pages currently in cache" : 0,
				"Leaf pages currently in cache" : 0,
				"Maximum difference between current eviction generation when the page was last considered" : 0,
				"Maximum page size seen" : 0,
				"Minimum on-disk page image size seen" : 0,
				"Number of pages never visited by eviction server" : 0,
				"On-disk page image sizes smaller than a single allocation unit" : 0,
				"Pages created in memory and never written" : 0,
				"Pages currently queued for eviction" : 0,
				"Pages that could not be queued for eviction" : 0,
				"Refs skipped during cache traversal" : 0,
				"Size of the root page" : 0,
				"Total number of pages currently in cache" : 0
			},
			"compression" : {
				"compressed pages read" : 0,
				"compressed pages written" : 0,
				"page written failed to compress" : 0,
				"page written was too small to compress" : 0
			},
			"cursor" : {
				"bulk-loaded cursor-insert calls" : 0,
				"close calls that result in cache" : 0,
				"create calls" : 1,
				"cursor operation restarted" : 0,
				"cursor-insert key and value bytes inserted" : 857035,
				"cursor-remove key bytes removed" : 0,
				"cursor-update value bytes updated" : 0,
				"cursors reused from cache" : 50,
				"insert calls" : 50474,
				"modify calls" : 0,
				"next calls" : 0,
				"open cursor count" : 0,
				"prev calls" : 0,
				"remove calls" : 0,
				"reserve calls" : 0,
				"reset calls" : 50525,
				"search calls" : 0,
				"search near calls" : 0,
				"truncate calls" : 0,
				"update calls" : 0
			},
			"reconciliation" : {
				"dictionary matches" : 0,
				"fast-path pages deleted" : 0,
				"internal page key bytes discarded using suffix compression" : 106,
				"internal page multi-block writes" : 0,
				"internal-page overflow keys" : 0,
				"leaf page key bytes discarded using prefix compression" : 604609,
				"leaf page multi-block writes" : 2,
				"leaf-page overflow keys" : 0,
				"maximum blocks required for a page" : 1,
				"overflow values written" : 0,
				"page checksum matches" : 0,
				"page reconciliation calls" : 3,
				"page reconciliation calls for eviction" : 0,
				"pages deleted" : 0
			},
			"session" : {
				"object compaction" : 0
			},
			"transaction" : {
				"update conflicts" : 0
			}
		},
		"job_1_address.state_1_first_name_1" : {
			"metadata" : {
				"formatVersion" : 8,
				"infoObj" : "{ \"v\" : 2, \"key\" : { \"job\" : 1, \"address.state\" : 1, \"first_name\" : 1 }, \"name\" : \"job_1_address.state_1_first_name_1\", \"ns\" : \"m201.people\" }"
			},
			"creationString" : "access_pattern_hint=none,allocation_size=4KB,app_metadata=(formatVersion=8,infoObj={ \"v\" : 2, \"key\" : { \"job\" : 1, \"address.state\" : 1, \"first_name\" : 1 }, \"name\" : \"job_1_address.state_1_first_name_1\", \"ns\" : \"m201.people\" }),assert=(commit_timestamp=none,read_timestamp=none),block_allocation=best,block_compressor=,cache_resident=false,checksum=on,colgroups=,collator=,columns=,dictionary=0,encryption=(keyid=,name=),exclusive=false,extractor=,format=btree,huffman_key=,huffman_value=,ignore_in_memory_cache_size=false,immutable=false,internal_item_max=0,internal_key_max=0,internal_key_truncate=true,internal_page_max=16k,key_format=u,key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=16k,leaf_value_max=0,log=(enabled=true),lsm=(auto_throttle=true,bloom=true,bloom_bit_count=16,bloom_config=,bloom_hash_count=8,bloom_oldest=false,chunk_count_limit=0,chunk_max=5GB,chunk_size=10MB,merge_custom=(prefix=,start_generation=0,suffix=),merge_max=15,merge_min=0),memory_page_image_max=0,memory_page_max=5MB,os_cache_dirty_max=0,os_cache_max=0,prefix_compression=true,prefix_compression_min=4,source=,split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,type=file,value_format=u",
			"type" : "file",
			"uri" : "statistics:table:index-19-3181334468023989821",
			"LSM" : {
				"bloom filter false positives" : 0,
				"bloom filter hits" : 0,
				"bloom filter misses" : 0,
				"bloom filter pages evicted from cache" : 0,
				"bloom filter pages read into cache" : 0,
				"bloom filters in the LSM tree" : 0,
				"chunks in the LSM tree" : 0,
				"highest merge generation in the LSM tree" : 0,
				"queries that could have benefited from a Bloom filter that did not exist" : 0,
				"sleep for LSM checkpoint throttle" : 0,
				"sleep for LSM merge throttle" : 0,
				"total size of bloom filters" : 0
			},
			"block-manager" : {
				"allocations requiring file extension" : 61,
				"blocks allocated" : 61,
				"blocks freed" : 0,
				"checkpoint size" : 954368,
				"file allocation unit size" : 4096,
				"file bytes available for reuse" : 0,
				"file magic number" : 120897,
				"file major version number" : 1,
				"file size in bytes" : 966656,
				"minor version number" : 0
			},
			"btree" : {
				"btree checkpoint generation" : 3361,
				"column-store fixed-size leaf pages" : 0,
				"column-store internal pages" : 0,
				"column-store variable-size RLE encoded values" : 0,
				"column-store variable-size deleted values" : 0,
				"column-store variable-size leaf pages" : 0,
				"fixed-record size" : 0,
				"maximum internal page key size" : 1474,
				"maximum internal page size" : 16384,
				"maximum leaf page key size" : 1474,
				"maximum leaf page size" : 16384,
				"maximum leaf page value size" : 7372,
				"maximum tree depth" : 3,
				"number of key/value pairs" : 0,
				"overflow pages" : 0,
				"pages rewritten by compaction" : 0,
				"row-store internal pages" : 0,
				"row-store leaf pages" : 0
			},
			"cache" : {
				"bytes currently in the cache" : 269722,
				"bytes dirty in the cache cumulative" : 0,
				"bytes read into cache" : 136060,
				"bytes written from cache" : 858146,
				"checkpoint blocked page eviction" : 0,
				"data source pages selected for eviction unable to be evicted" : 0,
				"eviction walk passes of a file" : 0,
				"eviction walk target pages histogram - 0-9" : 0,
				"eviction walk target pages histogram - 10-31" : 0,
				"eviction walk target pages histogram - 128 and higher" : 0,
				"eviction walk target pages histogram - 32-63" : 0,
				"eviction walk target pages histogram - 64-128" : 0,
				"eviction walks abandoned" : 0,
				"eviction walks gave up because they restarted their walk twice" : 0,
				"eviction walks gave up because they saw too many pages and found no candidates" : 0,
				"eviction walks gave up because they saw too many pages and found too few candidates" : 0,
				"eviction walks reached end of tree" : 0,
				"eviction walks started from root of tree" : 0,
				"eviction walks started from saved location in tree" : 0,
				"hazard pointer blocked page eviction" : 0,
				"in-memory page passed criteria to be split" : 0,
				"in-memory page splits" : 0,
				"internal pages evicted" : 0,
				"internal pages split during eviction" : 0,
				"leaf pages split during eviction" : 1,
				"modified pages evicted" : 2,
				"overflow pages read into cache" : 0,
				"page split during eviction deepened the tree" : 0,
				"page written requiring cache overflow records" : 0,
				"pages read into cache" : 11,
				"pages read into cache after truncate" : 0,
				"pages read into cache after truncate in prepare state" : 0,
				"pages read into cache requiring cache overflow entries" : 0,
				"pages requested from the cache" : 180,
				"pages seen by eviction walk" : 0,
				"pages written from cache" : 60,
				"pages written requiring in-memory restoration" : 0,
				"tracked dirty bytes in the cache" : 0,
				"unmodified pages evicted" : 0
			},
			"cache_walk" : {
				"Average difference between current eviction generation when the page was last considered" : 0,
				"Average on-disk page image size seen" : 0,
				"Average time in cache for pages that have been visited by the eviction server" : 0,
				"Average time in cache for pages that have not been visited by the eviction server" : 0,
				"Clean pages currently in cache" : 0,
				"Current eviction generation" : 0,
				"Dirty pages currently in cache" : 0,
				"Entries in the root page" : 0,
				"Internal pages currently in cache" : 0,
				"Leaf pages currently in cache" : 0,
				"Maximum difference between current eviction generation when the page was last considered" : 0,
				"Maximum page size seen" : 0,
				"Minimum on-disk page image size seen" : 0,
				"Number of pages never visited by eviction server" : 0,
				"On-disk page image sizes smaller than a single allocation unit" : 0,
				"Pages created in memory and never written" : 0,
				"Pages currently queued for eviction" : 0,
				"Pages that could not be queued for eviction" : 0,
				"Refs skipped during cache traversal" : 0,
				"Size of the root page" : 0,
				"Total number of pages currently in cache" : 0
			},
			"compression" : {
				"compressed pages read" : 0,
				"compressed pages written" : 0,
				"page written failed to compress" : 0,
				"page written was too small to compress" : 0
			},
			"cursor" : {
				"bulk-loaded cursor-insert calls" : 50474,
				"close calls that result in cache" : 0,
				"create calls" : 3,
				"cursor operation restarted" : 0,
				"cursor-insert key and value bytes inserted" : 0,
				"cursor-remove key bytes removed" : 0,
				"cursor-update value bytes updated" : 0,
				"cursors reused from cache" : 1,
				"insert calls" : 0,
				"modify calls" : 0,
				"next calls" : 188,
				"open cursor count" : 0,
				"prev calls" : 69,
				"remove calls" : 0,
				"reserve calls" : 0,
				"reset calls" : 8,
				"search calls" : 0,
				"search near calls" : 180,
				"truncate calls" : 0,
				"update calls" : 0
			},
			"reconciliation" : {
				"dictionary matches" : 0,
				"fast-path pages deleted" : 0,
				"internal page key bytes discarded using suffix compression" : 803,
				"internal page multi-block writes" : 0,
				"internal-page overflow keys" : 0,
				"leaf page key bytes discarded using prefix compression" : 1372041,
				"leaf page multi-block writes" : 1,
				"leaf-page overflow keys" : 0,
				"maximum blocks required for a page" : 0,
				"overflow values written" : 0,
				"page checksum matches" : 0,
				"page reconciliation calls" : 1,
				"page reconciliation calls for eviction" : 1,
				"pages deleted" : 0
			},
			"session" : {
				"object compaction" : 0
			},
			"transaction" : {
				"update conflicts" : 0
			}
		},
		"job_1" : {
			"metadata" : {
				"formatVersion" : 8,
				"infoObj" : "{ \"v\" : 2, \"key\" : { \"job\" : 1 }, \"name\" : \"job_1\", \"ns\" : \"m201.people\" }"
			},
			"creationString" : "access_pattern_hint=none,allocation_size=4KB,app_metadata=(formatVersion=8,infoObj={ \"v\" : 2, \"key\" : { \"job\" : 1 }, \"name\" : \"job_1\", \"ns\" : \"m201.people\" }),assert=(commit_timestamp=none,read_timestamp=none),block_allocation=best,block_compressor=,cache_resident=false,checksum=on,colgroups=,collator=,columns=,dictionary=0,encryption=(keyid=,name=),exclusive=false,extractor=,format=btree,huffman_key=,huffman_value=,ignore_in_memory_cache_size=false,immutable=false,internal_item_max=0,internal_key_max=0,internal_key_truncate=true,internal_page_max=16k,key_format=u,key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=16k,leaf_value_max=0,log=(enabled=true),lsm=(auto_throttle=true,bloom=true,bloom_bit_count=16,bloom_config=,bloom_hash_count=8,bloom_oldest=false,chunk_count_limit=0,chunk_max=5GB,chunk_size=10MB,merge_custom=(prefix=,start_generation=0,suffix=),merge_max=15,merge_min=0),memory_page_image_max=0,memory_page_max=5MB,os_cache_dirty_max=0,os_cache_max=0,prefix_compression=true,prefix_compression_min=4,source=,split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,type=file,value_format=u",
			"type" : "file",
			"uri" : "statistics:table:index-22-3181334468023989821",
			"LSM" : {
				"bloom filter false positives" : 0,
				"bloom filter hits" : 0,
				"bloom filter misses" : 0,
				"bloom filter pages evicted from cache" : 0,
				"bloom filter pages read into cache" : 0,
				"bloom filters in the LSM tree" : 0,
				"chunks in the LSM tree" : 0,
				"highest merge generation in the LSM tree" : 0,
				"queries that could have benefited from a Bloom filter that did not exist" : 0,
				"sleep for LSM checkpoint throttle" : 0,
				"sleep for LSM merge throttle" : 0,
				"total size of bloom filters" : 0
			},
			"block-manager" : {
				"allocations requiring file extension" : 21,
				"blocks allocated" : 21,
				"blocks freed" : 0,
				"checkpoint size" : 299008,
				"file allocation unit size" : 4096,
				"file bytes available for reuse" : 0,
				"file magic number" : 120897,
				"file major version number" : 1,
				"file size in bytes" : 311296,
				"minor version number" : 0
			},
			"btree" : {
				"btree checkpoint generation" : 3361,
				"column-store fixed-size leaf pages" : 0,
				"column-store internal pages" : 0,
				"column-store variable-size RLE encoded values" : 0,
				"column-store variable-size deleted values" : 0,
				"column-store variable-size leaf pages" : 0,
				"fixed-record size" : 0,
				"maximum internal page key size" : 1474,
				"maximum internal page size" : 16384,
				"maximum leaf page key size" : 1474,
				"maximum leaf page size" : 16384,
				"maximum leaf page value size" : 7372,
				"maximum tree depth" : 3,
				"number of key/value pairs" : 0,
				"overflow pages" : 0,
				"pages rewritten by compaction" : 0,
				"row-store internal pages" : 0,
				"row-store leaf pages" : 0
			},
			"cache" : {
				"bytes currently in the cache" : 42495,
				"bytes dirty in the cache cumulative" : 0,
				"bytes read into cache" : 15421,
				"bytes written from cache" : 268175,
				"checkpoint blocked page eviction" : 0,
				"data source pages selected for eviction unable to be evicted" : 0,
				"eviction walk passes of a file" : 0,
				"eviction walk target pages histogram - 0-9" : 0,
				"eviction walk target pages histogram - 10-31" : 0,
				"eviction walk target pages histogram - 128 and higher" : 0,
				"eviction walk target pages histogram - 32-63" : 0,
				"eviction walk target pages histogram - 64-128" : 0,
				"eviction walks abandoned" : 0,
				"eviction walks gave up because they restarted their walk twice" : 0,
				"eviction walks gave up because they saw too many pages and found no candidates" : 0,
				"eviction walks gave up because they saw too many pages and found too few candidates" : 0,
				"eviction walks reached end of tree" : 0,
				"eviction walks started from root of tree" : 0,
				"eviction walks started from saved location in tree" : 0,
				"hazard pointer blocked page eviction" : 0,
				"in-memory page passed criteria to be split" : 0,
				"in-memory page splits" : 0,
				"internal pages evicted" : 0,
				"internal pages split during eviction" : 0,
				"leaf pages split during eviction" : 1,
				"modified pages evicted" : 2,
				"overflow pages read into cache" : 0,
				"page split during eviction deepened the tree" : 0,
				"page written requiring cache overflow records" : 0,
				"pages read into cache" : 2,
				"pages read into cache after truncate" : 0,
				"pages read into cache after truncate in prepare state" : 0,
				"pages read into cache requiring cache overflow entries" : 0,
				"pages requested from the cache" : 2,
				"pages seen by eviction walk" : 0,
				"pages written from cache" : 20,
				"pages written requiring in-memory restoration" : 0,
				"tracked dirty bytes in the cache" : 0,
				"unmodified pages evicted" : 0
			},
			"cache_walk" : {
				"Average difference between current eviction generation when the page was last considered" : 0,
				"Average on-disk page image size seen" : 0,
				"Average time in cache for pages that have been visited by the eviction server" : 0,
				"Average time in cache for pages that have not been visited by the eviction server" : 0,
				"Clean pages currently in cache" : 0,
				"Current eviction generation" : 0,
				"Dirty pages currently in cache" : 0,
				"Entries in the root page" : 0,
				"Internal pages currently in cache" : 0,
				"Leaf pages currently in cache" : 0,
				"Maximum difference between current eviction generation when the page was last considered" : 0,
				"Maximum page size seen" : 0,
				"Minimum on-disk page image size seen" : 0,
				"Number of pages never visited by eviction server" : 0,
				"On-disk page image sizes smaller than a single allocation unit" : 0,
				"Pages created in memory and never written" : 0,
				"Pages currently queued for eviction" : 0,
				"Pages that could not be queued for eviction" : 0,
				"Refs skipped during cache traversal" : 0,
				"Size of the root page" : 0,
				"Total number of pages currently in cache" : 0
			},
			"compression" : {
				"compressed pages read" : 0,
				"compressed pages written" : 0,
				"page written failed to compress" : 0,
				"page written was too small to compress" : 0
			},
			"cursor" : {
				"bulk-loaded cursor-insert calls" : 50474,
				"close calls that result in cache" : 0,
				"create calls" : 2,
				"cursor operation restarted" : 0,
				"cursor-insert key and value bytes inserted" : 0,
				"cursor-remove key bytes removed" : 0,
				"cursor-update value bytes updated" : 0,
				"cursors reused from cache" : 0,
				"insert calls" : 0,
				"modify calls" : 0,
				"next calls" : 0,
				"open cursor count" : 0,
				"prev calls" : 69,
				"remove calls" : 0,
				"reserve calls" : 0,
				"reset calls" : 3,
				"search calls" : 0,
				"search near calls" : 2,
				"truncate calls" : 0,
				"update calls" : 0
			},
			"reconciliation" : {
				"dictionary matches" : 0,
				"fast-path pages deleted" : 0,
				"internal page key bytes discarded using suffix compression" : 42,
				"internal page multi-block writes" : 0,
				"internal-page overflow keys" : 0,
				"leaf page key bytes discarded using prefix compression" : 1242529,
				"leaf page multi-block writes" : 1,
				"leaf-page overflow keys" : 0,
				"maximum blocks required for a page" : 0,
				"overflow values written" : 0,
				"page checksum matches" : 0,
				"page reconciliation calls" : 1,
				"page reconciliation calls for eviction" : 1,
				"pages deleted" : 0
			},
			"session" : {
				"object compaction" : 0
			},
			"transaction" : {
				"update conflicts" : 0
			}
		}
	},
	"totalIndexSize" : 1740800,
	"indexSizes" : {
		"_id_" : 462848,
		"job_1_address.state_1_first_name_1" : 966656,
		"job_1" : 311296
	},
	"ok" : 1
}
```

This returns a SIGNIFICANT amount of data. The most interesting field being `cache`:
```
"cache" : {
				"bytes currently in the cache" : 42495,
				"bytes dirty in the cache cumulative" : 0,
				"bytes read into cache" : 15421,
				"bytes written from cache" : 268175,
				"checkpoint blocked page eviction" : 0,
				"data source pages selected for eviction unable to be evicted" : 0,
				"eviction walk passes of a file" : 0,
				"eviction walk target pages histogram - 0-9" : 0,
				"eviction walk target pages histogram - 10-31" : 0,
				"eviction walk target pages histogram - 128 and higher" : 0,
				"eviction walk target pages histogram - 32-63" : 0,
				"eviction walk target pages histogram - 64-128" : 0,
				"eviction walks abandoned" : 0,
				"eviction walks gave up because they restarted their walk twice" : 0,
				"eviction walks gave up because they saw too many pages and found no candidates" : 0,
				"eviction walks gave up because they saw too many pages and found too few candidates" : 0,
				"eviction walks reached end of tree" : 0,
				"eviction walks started from root of tree" : 0,
				"eviction walks started from saved location in tree" : 0,
				"hazard pointer blocked page eviction" : 0,
				"in-memory page passed criteria to be split" : 0,
				"in-memory page splits" : 0,
				"internal pages evicted" : 0,
				"internal pages split during eviction" : 0,
				"leaf pages split during eviction" : 1,
				"modified pages evicted" : 2,
				"overflow pages read into cache" : 0,
				"page split during eviction deepened the tree" : 0,
				"page written requiring cache overflow records" : 0,
				"pages read into cache" : 2,
				"pages read into cache after truncate" : 0,
				"pages read into cache after truncate in prepare state" : 0,
				"pages read into cache requiring cache overflow entries" : 0,
				"pages requested from the cache" : 2,
				"pages seen by eviction walk" : 0,
				"pages written from cache" : 20,
				"pages written requiring in-memory restoration" : 0,
				"tracked dirty bytes in the cache" : 0,
				"unmodified pages evicted" : 0
			},
```

When starting wiredtiger storage engine, can specify the index cache size using `--wiredTigerCacheSizeGB` command line option.

## Edge Cases
It is typical to have indexes all in memory, but there are some edge cases where this can not be met:
* Analysis Queries (reports)
* Indexes Fields that grow monotonically like counters and date values (IoT)

#### Reports/Analytics
It is such that these will typically be ran specifically on non-primary nodes within a database cluster, using indexes on that secondary node, which are unlikely to fit within RAM.

#### Monotonically
Typically, with these indexes we're only operating (querying) on  most subset (typically most recent) set of data, and thus only part of the index needs to be held in memory.

## Benchmarking
Types of performance benchmarking:
* Public test suite
* Specific of private testing environment

### Low Level Benchmarking
Typically done by a vendor and Includes:
* File IO
* Scheduler
* Memory Allocation/transfer speeds
* Thread performance
* Database server performance
* Transaction isoation

### Database Benchmarking
Also typically done by a vendor, but also by system providers (comparing say a small, medium and large cloud server instance) and includes test specific to how a database performs, including:
* Data set load
* Writes per second
* Reads per second
* Balance workloads
* Read/Write ratio

Indusrty benchmarks also exist ([TPC](http://www.tpc.org/information/benchmarks.asp)).

### Distributed Systems Benchmarking
This extends the tests across multiple nodes and includes:
* Linearization
* Serialisation
* Fault Tolerance/Recovery

### Custom
A friend of MongoDB has written an unofficial Java client stimulator for measure MongoDB performance: https://github.com/johnlpage/POCDriver.

### Anti-Patterns
Any benchmarks can be mis-representing, because the may not reflect you application's behaviour. The data model has significant impact on performance.

Trying to directly compare benchmarks between an RDBMS and MongoDB is not valid, because the data model will not be the same, and the use on collections across collections not the same.

Using `mongo` shell and `mongoimport` are not valid. The shell is written with some optimisations but is also a single access point. `mongoimport` has been written with bulk load in mind - and is not how a typically application would run.

And when doing tests, use suitable test hardware. Running test from a laptop are worthless (slower CPUs, slower disks, other processes running). And running tests from a single client worthless in a distributed server environment as the client's network IO will become a point of connection.

And need to tune `mongod`/`mongos` default parameters repeating the tests every time.
# Exercises
## Index Prefixes
Given the following three queries, which of the compound indexes below will yield the best performance across all three:

1.
```
db.people.find({
    "address.state": "Nebraska",
    "last_name": /^G/,
    "job": "Police officer"
  }).explain("executionStats")
```


2.
```
db.people.find({
    "job": /^P/,
    "first_name": /^C/,
    "address.state": "Indiana"
  }).sort({ "last_name": 1 }).explain("executionStats")
```

3.
```
db.people.find({
    "address.state": "Connecticut",
    "birthday": {
      "$gte": ISODate("2010-01-01T00:00:00.000Z"),
      "$lt": ISODate("2011-01-01T00:00:00.000Z")
    }
  }).explain("executionStats")
```


---------------

1.`db.people.createIndex({job:1, "address.state": 1, "last_name": 1})`
```
2.1.1 - IXSCAN - 4keys and 2docs - 2docs returned
1.2 - SORT, then FETCH on IXSCAN:
    "nReturned" : 8,
		"executionTimeMillis" : 2,
		"totalKeysExamined" : 204,
		"totalDocsExamined" : 67,
1.3 - COLSCAN
		"nReturned" : 96,
		"executionTimeMillis" : 18,
		"totalKeysExamined" : 0,
		"totalDocsExamined" : 50474,
```


3. `db.people.createIndex({"job":1, "address.state":1})`
```
3.1. IXSCAN
  	"nReturned" : 2,
		"executionTimeMillis" : 1,
		"totalKeysExamined" : 3,
		"totalDocsExamined" : 3,

3.2. SORT/FETCH/IXSCAN
		"nReturned" : 8,
		"executionTimeMillis" : 3,
		"totalKeysExamined" : 204,
		"totalDocsExamined" : 67,

3.3. COLSCAN
		"nReturned" : 96,
		"executionTimeMillis" : 19,
		"totalKeysExamined" : 0,
		"totalDocsExamined" : 50474,
```

6. `db.people.createIndex({"job":1, "address.state":1, "first_name": 1})`
```
7.6.1. IXSCAN
		"nReturned" : 2,
		"executionTimeMillis" : 1,
		"totalKeysExamined" : 3,
		"totalDocsExamined" : 3,
6.2 SORT/FETCH/IXSCAN
		"nReturned" : 8,
		"executionTimeMillis" : 3,
		"totalKeysExamined" : 183,
		"totalDocsExamined" : 8,
6.3 COLSCAN
		"nReturned" : 96,
		"executionTimeMillis" : 18,
		"totalKeysExamined" : 0,
		"totalDocsExamined" : 50474,
```

4. `db.people.createIndex({"address.state":1, "job":1})`
```
4.1 IXSCAN
		"nReturned" : 2,
		"executionTimeMillis" : 1,
		"totalKeysExamined" : 3,
		"totalDocsExamined" : 3,
4.2 SORT/FETCH/IXSCAN
		"nReturned" : 8,
		"executionTimeMillis" : 1,
		"totalKeysExamined" : 69,
		"totalDocsExamined" : 67,
4.3 FETCH/IXSCAN
		"nReturned" : 96,
		"executionTimeMillis" : 4,
		"totalKeysExamined" : 716,
		"totalDocsExamined" : 716,
```

> I chose this one based on the facts:
5. `db.people.createIndex({"address.state":1, "job":1, "first_name": 1})`
```
5.1 IXSCAN
		"nReturned" : 2,
		"executionTimeMillis" : 1,
		"totalKeysExamined" : 3,
		"totalDocsExamined" : 3,
5.2 SORT/FETCH/IXSCAN
		"nReturned" : 8,
		"executionTimeMillis" : 0,
		"totalKeysExamined" : 59,
		"totalDocsExamined" : 8,
5.3 FETCH/SCAN
		"nReturned" : 96,
		"executionTimeMillis" : 4,
		"totalKeysExamined" : 716,
		"totalDocsExamined" : 716,
```

----
2. `db.people.createIndex({"address.state": 1, "last_name": 1, "job":1})`
```
2.1 - IXSCAN -
    "nReturned" : 2,
		"executionTimeMillis" : 1,
		"totalKeysExamined" : 31,
		"totalDocsExamined" : 2,
2.2 - FETCH/IXSCAN
		"nReturned" : 8,
		"executionTimeMillis" : 3,
		"totalKeysExamined" : 488,
		"totalDocsExamined" : 67,
2.3 - FETCH/IXSCAN
		"nReturned" : 96,
		"executionTimeMillis" : 4,
		"totalKeysExamined" : 716,
		"totalDocsExamined" : 716,
```