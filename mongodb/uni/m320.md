<!-- TITLE: M320 -->
<!-- SUBTITLE: M320 Data Modelling -->

# Prerequisites
* Crows Feet Data Modelling notation: http://www2.cs.uregina.ca/~bernatja/crowsfoot.html.
* Nice blog: https://www.mongodb.com/blog/post/6-rules-of-thumb-for-mongodb-schema-design-part-1 by William Zola of MongoDB.

# Overview
A flexible data model; only prereq is a primary key and data is stored in BSON.

The fact that MongoDB is schemaless is more a benefit than a requirement. Schema (document validation) can be associated to a collection; a schema itself can be flexible.

When considering the data model, you must consider:
1. Usage (of data) patterns
2. How data is accessed (reads and writes)
3. Which queries are critical
4. Ratios between reads and writes

And remember, these change over time, and therefore, the flexible model of MongoDB allows for changes over time.

Although historically, the MongoDB approach has been, stored everything in one document (up to 16MB), with the introduction of `$lookup`, many to many relationships can be better supported.

Note also, as noted [here - which may not still be valid](https://www.infoq.com/articles/Starting-With-MongoDB/), keep the number of array items within a single document within four digits.

# Introduction
## MongoDB Document Model
* Many Databases
	* Many Collection
		* Many documents

Documents are JSON (key:value pairs); stored as BSON. JSON has few data types, BSON has more defined [data types](https://docs.mongodb.com/manual/reference/bson-types/).

Document validation can be none, on predefined fields or on all fields of a document.

## Constraints
* Hardware (RAM/Disk)
* Data (size, security, [sovereignty](https://rgtechnologies.com.au/resources/data-sovereignty/)
* Application Network latency
* Application Update atomicity
* MongoDB itself - document max 16MB


`Working Set` - frequently accessed documents and indexes (RAM). Depends on whether NMAP or WireTiger, but typically is 50% of the available RAM. This is impacted by the size of documents and the number of documents, and significantly includes the indexes of those documents!

Some tips:
* Keep the frequently used documents in RAM
* Keep the indexes in RAM
* SSD over HDD

## Methodology
![Data Modeling Methodology](/uploads/mongodb/methodology.png "MongoDB Data Modelling Methodology")

First, look at the workload which includes:
1. Size of data
2. Number of reads and writes
3. What types of reads, and what types of writes, especially temporal

Second, identify the relationships and properties:
1. Identify
2. Quantity
3. Qualify - one to many, zero to many, one to one, capped, reuse, duplicate
4. Embed or link (cross-ref)

Apply patterns; there are a number of recipes can that be utilised for common _data_ situations.

Repeat. Review. Repeat. Review. Repeat....

### Simplicity vs Performance
Model for simplicity or for performance. Iterative modelling on simplicity and then performance back to simplicity and then again for performance.

Simplicity would be few collections with embedded documents, supporting the most frequent queries. Atypical of small teams and small projects.

Performance requires analysis and considerations of #reads/#writes, latency, read/write concerns, sharding and workload isolation. Atypical of large projects and large teams, with data and knowledge experts.

# Identitfying Workload
![IoT Example Topology](/uploads/mongodb/iot-topology.png "IoT Example Topology")

Record the scenarios, e.g. "how many devices", "how often", "the users", "retention", "location", resilience, security, threats, ...

Quantity and Qualify the operations, size of device data, the device data that will be sent, whether it's a read or a write,...

Start with the most important (significant) operation, and describe in more detail that operation:
![For Each Write Operation](/uploads/mongodb/for-each-operation.png "Detailed on Each Write Operation")

![For Each Read Operation](/uploads/mongodb/for-each-read-operation.png "Detailed on Each Read Operation")


[M320 IoT Workload Analysis](/uploads/mongodb/m-320-workload-iot.pdf "M320 IoT Workload Analysis")

# Identifying the Relationships
Cardinality:
* 1 to 1
* 1 to many
* Many to many

With the "1 to many", the relationship is further clarified by the "number of many", but also the ownership of that many, and even the cardinality relationships of the document at the end of the many.

For example, a mother has children; typically least than 10 children. Here, the cardinality, still many, is low, but moreso, the ownership of the relationship is with the mother. As an alternative, consider a mother and her friends; on a social media network, this could be hundreds or thousands and the mother does not own the relationship, especially as it (the friendship) can be broken.

In addition to the many, zero and one modifiers, modified notations:
* one to zillions - not just three crows feet but five crows feet.
* [minimum, most likley/median, maximum] tuple

![Modified Notation](/uploads/mongodb/modified-notation.png "Modified Notation")

## 1 to Many
If the relationship is owned (once created is not broken), typically embedded into the document that is accessed most frequently, either:
* Embed array of the many within the one document - most frequent
* Embed the one within each of the many documents - the one document is duplicated

If the relationship is not owned, typically reference, again in the document that is accessed most frequently:
* Link array of the many within the one document - where the associated set of referred documents is subject to change
* Link the one within each of the many documents - where the document being referenced is static

When not to embed:
* if the cardinality of the many is high, or constant change
* if the size of the embedded data is large
* if duplication is not an issue (when embedding the one in the many)
* confidentiality of the data if different to that of the owner - e.g. GDPR and say email address.

## Many to Many
Effectively, a special case on the one to many, where the many cardinality is bidirectional. With duplication, a many to many can be reduced (simplified) to a one to many relationship. The example given being a home telephone number is shared by all family members, but is simpler to represent as a one to many, where the home phone number is duplicated.

If the relationship is owned:
* Embed an array - in whichever document is queried most frequently - but not both?

If the relationship is not owned (and can easily change):
* Link an array - in whichever document is queried most frequently - but not both?

With many to many, when trying to decide which document to embed/link the array, look not just at the queries, but also the updates.

When not to embed:
* if duplication is not good; for example, the need to forget personal data
* embedded document  size
* frequency embedded data is accessed is very low

## One to One
If the relationship is owned:
* Embed - as a field at the same level, within the document that is accessed most frequently, for example, name, age, quantity
* Group - associated relationships in a single sub document, within the document that is accessed most frequently, for example "address" is a compound document

If the relationship not owned:
* Reference - same identifier in both documents
* Reference - just one in the 'parent'
* Reference - just one in the 'child'

## One to a Zillion
Here, of the earlier one to many approaches, only one is valid:
* Link the one within each of the many (traditional relational approach)

# Patterns
Utilise good practice from others; a cookbook/recipes for more predictable outcomes.

## validator
The M320 course comes with a validator binary `validate_m320`, which will be used in this course to _mark/grade_ homework.

To run - `./validate_m320 example -- answer_schema.json`.

## Cost of Patterns
Applying patterns may result in:
1. Duplication
2. Data Staleness
3. Data Integrity relaxation - requiring extra applicatoin logic to assure integrity

## Duplication
Is typically a result of embedding data for faster access. The concern here is correctness and consistency of the duplicated data.

There are times when duplication is better; for example, a shipping address. If the customer's address is updated after an order is shipped, will result in a the shipping address of all previous orders being also updated.

Even when discrepancies between duplication exists, should evaluate whether inconsistency of data or the effort involved in resetting all duplicated data (or indeed, calculated and stored data) is worth it.

## Staleness
As result of when events on data come at a high rate (higher than read/computed).

Staleness can be corrected in batch. But now MongoDB change streams can be consumed to apply more efficient updates to stale data (which note, can still be batched - but at least the batch is on documents that are known to require updates).

## Integrity
This includes both correcting existing references, but also, removing references to documents no longer existing (cascade deletes not supported in MongoDB).

To maintain integrity can:
1. Use change streams to force a reset/recalc on references
2. Embed data rather than reference
3. Use V4.0 transactions


# Patterns
## Attribute Pattern
One of the most popular _polymorhpic_ patterns; **orthogonal (transpose) pattern to polymorphism**.

A product will have fields describing its characteristics, such as size and quantity. Size for each product will vary in units and reason. The example given was in USA, the capacity of drink can are measured in `oz` whereas in Europe it will be in `ml` and the "size" of battery charger the units the be in `mm` but will be width, height and depth - the physical size. A drink can may be sold individually or as a pack, and therefore may have a quantity field; charger is likely to be individually. For storage concern, the drink can will also need physical measurements - which will vary also on whether the drink is sold individually (e.g. a tray of) or as a pack. And the charger will have fields not applicable to can of drink, such as, power capacity. 

A product may have a few fields or many fields; to be able to search on each charactertics, there needs to be a index on each field - and these indexes will need to be managed overtime. _V4.2 supports wildcard indexes makes managing indexes easier._

Instead of having such characteristics as fields, have single field being an array of key/value pairs, as demonstrated in the slide below (note _add_specs_ in meant to refer to _additional specifications_):
![Attribute Pattern](/uploads/mongodb/mongodb-attribute-pattern.png "Attribute Pattern")

The attribute pattern can also be applied to fields that share common characteristics; especially if wanting to be able to search across those fields - but also the opposite, it can be used to group rare/esoteric characteristics.

The attribute pattern addresses:
* Problem:
	* Lots of similar fields
	* Want to search across many fields at once
	* Fields present in only a small subset of documents
* Solution:
	* Break the field/value into a sub-document or key/values with:
		* fieldA: _the field name_
		* fieldB:  _the field value_
	* The field value can be a document; does not have to be a simple type.
* Use cases:
	* Characteristics of products
	* Sets of fields all having the same value type (e.g. list of dates).
* Benefits and Trade Offs
	* Easier to index - index only on the sub-document, not each field
	* Allows for non-deterministic field names (can be added at anytime)

