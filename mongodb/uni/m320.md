<!-- TITLE: M320 -->
<!-- SUBTITLE: M320 Data Modelling -->

# Prerequisites
* Crows Feet Data Modelling notation: http://www2.cs.uregina.ca/~bernatja/crowsfoot.html.
* Nice blog: https://www.mongodb.com/blog/post/6-rules-of-thumb-for-mongodb-schema-design-part-1 by William Zola of MongoDB.

# Overview
A flexible data model; only prereq is a primary key and data is stored in BSON.

The fact that MongoDB is schemaless is more a benefit than a requirement. Schema (document validation) can be associated to a collection; a schema itself can be flexible.

When considering the data model, you must consider:
1. Usage (of data) patterns
2. How data is accessed (reads and writes)
3. Which queries are critical
4. Ratios between reads and writes

And remember, these change over time, and therefore, the flexible model of MongoDB allows for changes over time.

Although historically, the MongoDB approach has been, stored everything in one document (up to 16MB), with the introduction of `$lookup`, many to many relationships can be better supported.

Note also, as noted [here - which may not still be valid](https://www.infoq.com/articles/Starting-With-MongoDB/), keep the number of array items within a single document within four digits.

# Introduction
## MongoDB Document Model
* Many Databases
	* Many Collection
		* Many documents

Documents are JSON (key:value pairs); stored as BSON. JSON has few data types, BSON has more defined [data types](https://docs.mongodb.com/manual/reference/bson-types/).

Document validation can be none, on predefined fields or on all fields of a document.

## Constraints
* Hardware (RAM/Disk)
* Data (size, security, [sovereignty](https://rgtechnologies.com.au/resources/data-sovereignty/)
* Application Network latency
* Application Update atomicity
* MongoDB itself - document max 16MB


`Working Set` - frequently accessed documents and indexes (RAM). Depends on whether NMAP or WireTiger, but typically is 50% of the available RAM. This is impacted by the size of documents and the number of documents, and significantly includes the indexes of those documents!

Some tips:
* Keep the frequently used documents in RAM
* Keep the indexes in RAM
* SSD over HDD

## Methodology
![Data Modeling Methodology](/uploads/mongodb/methodology.png "MongoDB Data Modelling Methodology")

First, look at the workload which includes:
1. Size of data
2. Number of reads and writes
3. What types of reads, and what types of writes, especially temporal

Second, identify the relationships and properties:
1. Identify
2. Quantity
3. Qualify - one to many, zero to many, one to one, capped, reuse, duplicate
4. Embed or link (cross-ref)

Apply patterns; there are a number of recipes can that be utilised for common _data_ situations.

Repeat. Review. Repeat. Review. Repeat....

### Simplicity vs Performance
Model for simplicity or for performance. Iterative modelling on simplicity and then performance back to simplicity and then again for performance.

Simplicity would be few collections with embedded documents, supporting the most frequent queries. Atypical of small teams and small projects.

Performance requires analysis and considerations of #reads/#writes, latency, read/write concerns, sharding and workload isolation. Atypical of large projects and large teams, with data and knowledge experts.

# Identitfying Workload
![IoT Example Topology](/uploads/mongodb/iot-topology.png "IoT Example Topology")

Record the scenarios, e.g. "how many devices", "how often", "the users", "retention", "location", resilience, security, threats, ...

Quantity and Qualify the operations, size of device data, the device data that will be sent, whether it's a read or a write,...

Start with the most important (significant) operation, and describe in more detail that operation:
![For Each Write Operation](/uploads/mongodb/for-each-operation.png "Detailed on Each Write Operation")

![For Each Read Operation](/uploads/mongodb/for-each-read-operation.png "Detailed on Each Read Operation")


[M320 IoT Workload Analysis](/uploads/mongodb/m-320-workload-iot.pdf "M320 IoT Workload Analysis")

# Identifying the Relationships
Cardinality:
* 1 to 1
* 1 to many
* Many to many

With the "1 to many", the relationship is further clarified by the "number of many", but also the ownership of that many, and even the cardinality relationships of the document at the end of the many.

For example, a mother has children; typically least than 10 children. Here, the cardinality, still many, is low, but moreso, the ownership of the relationship is with the mother. As an alternative, consider a mother and her friends; on a social media network, this could be hundreds or thousands and the mother does not own the relationship, especially as it (the friendship) can be broken.

In addition to the many, zero and one modifiers, modified notations:
* one to zillions - not just three crows feet but five crows feet.
* [minimum, most likley/median, maximum] tuple

![Modified Notation](/uploads/mongodb/modified-notation.png "Modified Notation")

## 1 to Many
If the relationship is owned (once created is not broken), typically embedded into the document that is accessed most frequently, either:
* Embed array of the many within the one document - most frequent
* Embed the one within each of the many documents - the one document is duplicated

If the relationship is not owned, typically reference, again in the document that is accessed most frequently:
* Link array of the many within the one document - where the associated set of referred documents is subject to change
* Link the one within each of the many documents - where the document being referenced is static

When not to embed:
* if the cardinality of the many is high, or constant change
* if the size of the embedded data is large
* if duplication is not an issue (when embedding the one in the many)
* confidentiality of the data if different to that of the owner - e.g. GDPR and say email address.

## Many to Many
Effectively, a special case on the one to many, where the many cardinality is bidirectional. With duplication, a many to many can be reduced (simplified) to a one to many relationship. The example given being a home telephone number is shared by all family members, but is simpler to represent as a one to many, where the home phone number is duplicated.

If the relationship is owned:
* Embed an array - in whichever document is queried most frequently - but not both?

If the relationship is not owned (and can easily change):
* Link an array - in whichever document is queried most frequently - but not both?

With many to many, when trying to decide which document to embed/link the array, look not just at the queries, but also the updates.

When not to embed:
* if duplication is not good; for example, the need to forget personal data
* embedded document  size
* frequency embedded data is accessed is very low

## One to One
If the relationship is owned:
* Embed - as a field at the same level, within the document that is accessed most frequently, for example, name, age, quantity
* Group - associated relationships in a single sub document, within the document that is accessed most frequently, for example "address" is a compound document

If the relationship not owned:
* Reference - same identifier in both documents
* Reference - just one in the 'parent'
* Reference - just one in the 'child'

## One to a Zillion
Here, of the earlier one to many approaches, only one is valid:
* Link the one within each of the many (traditional relational approach)

# Patterns
Utilise good practice from others; a cookbook/recipes for more predictable outcomes.

## validator
The M320 course comes with a validator binary `validate_m320`, which will be used in this course to _mark/grade_ homework.

To run - `./validate_m320 example -- answer_schema.json`.

## Cost of Patterns
Applying patterns may result in:
1. Duplication
2. Data Staleness
3. Data Integrity relaxation - requiring extra applicatoin logic to assure integrity

## Duplication
Is typically a result of embedding data for faster access. The concern here is correctness and consistency of the duplicated data.

There are times when duplication is better; for example, a shipping address. If the customer's address is updated after an order is shipped, will result in a the shipping address of all previous orders being also updated.

Even when discrepancies between duplication exists, should evaluate whether inconsistency of data or the effort involved in resetting all duplicated data (or indeed, calculated and stored data) is worth it.

## Staleness
As result of when events on data come at a high rate (higher than read/computed).

Staleness can be corrected in batch. But now MongoDB change streams can be consumed to apply more efficient updates to stale data (which note, can still be batched - but at least the batch is on documents that are known to require updates).

## Integrity
This includes both correcting existing references, but also, removing references to documents no longer existing (cascade deletes not supported in MongoDB).

To maintain integrity can:
1. Use change streams to force a reset/recalc on references
2. Embed data rather than reference
3. Use V4.0 transactions


## Attribute Pattern
One of the most popular _polymorhpic_ patterns; **orthogonal (transpose) pattern to polymorphism**.

A product will have fields describing its characteristics, such as size and quantity. Size for each product will vary in units and reason. The example given was in USA, the capacity of drink can are measured in `oz` whereas in Europe it will be in `ml` and the "size" of battery charger the units the be in `mm` but will be width, height and depth - the physical size. A drink can may be sold individually or as a pack, and therefore may have a quantity field; charger is likely to be individually. For storage concern, the drink can will also need physical measurements - which will vary also on whether the drink is sold individually (e.g. a tray of) or as a pack. And the charger will have fields not applicable to can of drink, such as, power capacity. 

A product may have a few fields or many fields; to be able to search on each charactertics, there needs to be a index on each field - and these indexes will need to be managed overtime. _V4.2 supports wildcard indexes makes managing indexes easier._

Instead of having such characteristics as fields, have single field being an array of key/value pairs, as demonstrated in the slide below (note _add_specs_ in meant to refer to _additional specifications_):
![Attribute Pattern](/uploads/mongodb/mongodb-attribute-pattern.png "Attribute Pattern")

The attribute pattern can also be applied to fields that share common characteristics; especially if wanting to be able to search across those fields - but also the opposite, it can be used to group rare/esoteric characteristics.

This "attribute" pattern addresses:
* Problem:
	* Lots of similar fields
	* Want to search across many fields at once
	* Fields present in only a small subset of documents
* Solution:
	* Break the field/value into a sub-document or key/values with:
		* fieldA: _the field name_
		* fieldB:  _the field value_
	* The field value can be a document; does not have to be a simple type.
* Use cases:
	* Characteristics of products
	* Sets of fields all having the same value type (e.g. list of dates).
* Benefits and Trade Offs
	* Easier to index - index only on the sub-document, not each field
	* Allows for non-deterministic field names (can be added at anytime)

## Extended Reference Pattern
Use this pattern to replace joins across tables, by embedding a copy of required reference data within the "many" document. A typical usage of this pattern is with Orders; an `Order` contains a referenece to the `Customer` and includes a field called _shippingAddress_, being just the address of the associated `Customer`.

This pattern works best by embedding only the fields required and those fields that don't change often.

And when the source document is updated, determine which of the "Extended Reference" pattern fields needs to be updated and how soon should the update happen.

Note - sometimes duplication is better; this is very much true of the _shipping address_ - which we don't want to change within the Order if the Customer's address changes at a later date (they move home).

This "Extended Reference" pattern addresses:
* Problem:
	* Too many repetitive joins (lookups)
* Solution
	* Identify the fields returned on the lookup
	* Embed those fields
* Uses Cases
	* Catalogue
	* Mobile Applications
	* Real Time Analytics
* Benefits and Trade Offs
	* Faster Reads
	* Reduced Number of joins
	* Duplication


## Subset Pattern
Reduces the size of individual documents, allow more documents within the "working set".

Moves those fields not accessed often, or having large arrays, move them to a new collection. Especially for arrays, you may only need to reference more frequently the top ten items in the array (for example, the most recent ten comments).

The "Subset" pattern addresses:
* Problem
	* Worker set is too big
	* Lots of RAM pages evicted from memory
* Solution
	* Split the collecition into two separate collections
		* Most used
		* Least used
* Uses Cases
	* List of reviews/comments
	* List of actors in a movie
* Benefits and Trade Offs
	* Smaller worki ng set
	* Short disk access (smaller docs)
	* More round trips, introducing joins/lookups

## Computed Pattern
Kinds of computations applied to data in documents include:
* Mathematical operations (sum, avg, standard deviation)
* Fan Out
	* on reads - one write,  resulting in multiple reads
	* on writes - multiples writes, but just one effective read
* Rollup (aggreated) - daily, weekly, monthly and yearly - or buckets - typified by a group operation

The computed pattern calculates the value on write, thus making it available on read. The calculated value could be written back to the same document or to a document in a new collection (for example which holds all computed relevant values).

If the number of writes is less than number of reads (for a given field), and the read latency having updated mutliple documents is acceptable, then "fan out on write" is preferred. If the number of writes is higher than reads, then "fan out on read" is preferred. An exmaple being a social media site and sharing of photos; write a link to the photo to all networked users.

* Problem:
	* Costly computation or majiuplation of data
	* Executed frequently on the same data producing the same result
* Solution
	* Perform the operation and stored the result in the _appropriate_ document/collection
	* Note - if needing to recalculate fairly frequently, think about keeping the source of the data with the document/collection ("Computed" and "Extended Reference" pattern)
* Use Cases
	* IOT
	* Event Sourcing
	* Time Series Data
	* Frequencty Aggregated pipelines
* Benefits/Trade Offs
	* Read queries are fatser
	* Saves on CPU/disk resource
	* May be difficult to identify the need
	* Avoid applying or overusing unless it is needed - can easily become a bottleneck not an optimsiation

## Bucket Pattern
The middle ground between "too fine" and "too course" documents. Atypified by IOT - generating a lot of data to be subsquently processed.

For example, create a document for each day for a single IOT device, or for a high change IOT device, one document per hour. Or indeed, can group multipe devices (similar type or region) together.

Bucket can be at document level, but can also be at collection level; separate collections for each bucket - e.g. all readings in January in one collection.

Buckets can be a combination of both collections and documents; all January readings with each document bucketing reading for devices of the same type.

Advantages of bucketing:
* Sharding data by the bucket key
* Bulk delete of data - can remove the January collection to remove all January's readings

In large datasets, dare warehouse (OLAP) schema have traditionally been used. Unlike their OLTP counterparts whcih store data by rows, OLAP is characterised most by storing data as columns. A MongoDB document having multiple array fields is both row and column data; having all data in a single document increases the size of the document. Yes, agreegation's `project` reduces the dataset in presented to a stage, but each document still needs to be returned and impacts on "working set". A separate document for each array field though, will _simulate_ (data is stored as like a column, but won't be indexed as a column like an OLAP) a warehouse schema, in just those documents having the required field are returned.

Buckets make it hard though when:
* Random inserts or deletes in buckets
* Harder to sort across buckets
* Adhoc queries more complex
* Works best when complexity is within the application.

* Problem
 * voiding to omany documents or too big documents
 * A 1-to-many relationship that can't be embedded
* Solution
 * Define the optimial amount of information to group (not, this can vary overtime to rememdber to review regularly)
* Use Cases
	* IOT
	* Data Warehousing
	* Lots of info associated to a single object
* Benefits/Trade off
	* Good balance between number of documents and size of documents
	* Data becomes more manageable - Sharding & bulk deletes
	* Can work against intended goal, in that query results are worse
	* Doesn't play nice with BI tools

## Schema Versioning Pattern
MongoDB has a flexible data model, and consquently documents change over time; new fields can be added. But there are times when fields are reorganised; consider if you will when you update the data model applying patterns.

Embed within the document the "schema_version". The application must be aware how to process each version of the document, and can use separate handlers for each version of the document, and ideally, the application should write back (update) all data to the latest version whenever updated. Overtime, the code for the old version can be removed from the application (knowing there are no more documents of the older version). Consider also a batch process to go through an update documents from one version to another a later version.

* Problem
	* Avoid downtime while doing schemaupgrades
	* Upgrading all documents can take hours, days or even weeks
	* Don't want to update everything
* Solution
 *  Each document gets a "schema_version" field
 *  Application must handle all versions
 *  Choose document migration strategy
* Use Cases
	*  Every applicatoin
	*  System with legacy data
	*  Agile environments
* Benefits/Trade Offs
	* No downtime needed
	* More in control of the migration
	* Less Tech Debt
	* Likely to need two indexes of the same field until completion of full migration.

## Tree Patterns
Hierarchical relationships - parent/child, typified with statements like:
* Who are the ancesters of X?
* Who report to X?
* Find all nodes that are under X?
* Change all categorgies under X to Y.

Tree models include:
1. Parent References
2. Child References
3. Array of Ancestors
4. Materialized Paths

### Parent References
This is where a child document refers back to its parent, for example:
```
{
	name: "Office",
	parent: "Department"
}
```

With this type of model, we can find all ancestors of using a graph lookup:
```
db.locations.aggregate([
	{
		$graphLookup: {
			from: "locations",
			startWith": "$name",
			connectFromField": "parent",
			connectToField: "name",
			as: "ancestors"
		}
	}
])
```

To list all who report to a given leaf with a simple find command: `db.locations.find({parent: "Department"})`.

And to change all categories, with a simple update: `db.categoies.updateMany({parent: "Stationary"}, {$set: { parent: "Office"}}`.

Although it is easy enough to lookup to get ancestors of a given leaf, the lookup is essential a join and will incur CPU/memory impact and consequently read latency.

### Child References
 Ths is where the parent document refers to all its child documents, for example:
 ```
 {
	 "name": "Office",
	 "children": ["Books", "Lectronics", "Stationary"]
 }
 ```
 
 Who are the ancestors, and all leafs that have a given parent are not so easy to answer with this approach, but finding all children and updating all children are simple and efficient actions.
 
 ### Array of Ancestors
 Here, every document includes an array listing ALL its ancestors; from the bottom to the top.
 
 For example, consider the topology of categories below, and look to the `ancestors` field on the left: 
 ![Mongodb Array Of Ancestors](/uploads/mongodb/mongodb-array-of-ancestors.png "Mongodb Array Of Ancestors")
 
 It (`ancestors`) includes the immediate parent leaf, "Office", but also the parent of "Office", namely "Swag":
 ```
 {
	 name: "Books",
	 ancestors: ["Office", "Swag"]
 }
 ```
 
 This model is efficeint finding all ancestors (no lookup) and who reports to; faster than that of "Parent References" alone. But finding all children and being able to update all children is harder still. And being able to find a specific leaf having a specific parent, harder still. The position of the elements with the array define the parent relationship.
 
 ### Materialized Paths
This is a slight variation on the "Array of Ancestors" model. Instead of having an array, the ancestors are represented using a dot path notation:
```
{
	 name: "Books",
	 ancestors: ".Swag.Office"
 }
```

Here, the parent of the leaf comes first. It is now not only easy to find ancestors (`$split` upon '.' to get the array of), but with a regex find can find all children, even those children of a specific parent hierarchy:
* `db.categories.find({ancestors: /\.Y$/})` where `Y` is the name of the leaf
* `db.categories.find({ancestors: /^\.X.*Y$/})` where `Y` is the name of the leaf (and must the last leaf in the hierarchy) and where `X` is the root parent
* `db.categories.find({ancestors: /\.X\.Y/})` where `Y` is the name of the child and `X` must be its parent

However, although these queries are simple to write, they are not performant because with the regex any index won't be used.

### Combining Models
But each of these models can be applied in combination to get the best of each.

* Problem
	* Representation of hierarchical structured data
	* Different access patterns to naviagte the three
	* Optimised model for common operations
* Solution
	* Child Reference
	* Parent Reference
	* Array of Ancestors
	* Matierialized Paths
* Use Cases:
	* Org Charts
	* Product Catlgoue
* Benefits/Trade Offs - choose the best model based on the most common operations
	* Child References
		* Easy to navigate child nodes or tree descendants
	* Parent References
	 * Immediate parnt node discovery and tree updates
	* Array of Ancestors
		* Navigate uipdates on the ancestors path
	* Materialized Paths
		* Uses regular expressions to find nodes in the tree


