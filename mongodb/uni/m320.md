<!-- TITLE: M320 -->
<!-- SUBTITLE: M320 Data Modelling -->

# Prerequisites
* Crows Feet Data Modelling notation: http://www2.cs.uregina.ca/~bernatja/crowsfoot.html.
* Nice blog: https://www.mongodb.com/blog/post/6-rules-of-thumb-for-mongodb-schema-design-part-1 by William Zola of MongoDB.

# Overview
A flexible data model; only prereq is a primary key and data is stored in BSON.

The fact that MongoDB is schemaless is more a benefit than a requirement. Schema (document validation) can be associated to a collection; a schema itself can be flexible.

When considering the data model, you must consider:
1. Usage (of data) patterns
2. How data is accessed (reads and writes)
3. Which queries are critical
4. Ratios between reads and writes

And remember, these change over time, and therefore, the flexible model of MongoDB allows for changes over time.

Although historically, the MongoDB approach has been, stored everything in one document (up to 16MB), with the introduction of `$lookup`, many to many relationships can be better supported.

Note also, as noted [here - which may not still be valid](https://www.infoq.com/articles/Starting-With-MongoDB/), keep the number of array items within a single document within four digits.

# Introduction
## MongoDB Document Model
* Many Databases
	* Many Collection
		* Many documents

Documents are JSON (key:value pairs); stored as BSON. JSON has few data types, BSON has more defined [data types](https://docs.mongodb.com/manual/reference/bson-types/).

Document validation can be none, on predefined fields or on all fields of a document.

## Constraints
* Hardware (RAM/Disk)
* Data (size, security, [sovereignty](https://rgtechnologies.com.au/resources/data-sovereignty/)
* Application Network latency
* Application Update atomicity
* MongoDB itself - document max 16MB


`Working Set` - frequently accessed documents and indexes (RAM). Depends on whether NMAP or WireTiger, but typically is 50% of the available RAM. This is impacted by the size of documents and the number of documents, and significantly includes the indexes of those documents!

Some tips:
* Keep the frequently used documents in RAM
* Keep the indexes in RAM
* SSD over HDD

## Methodology
![Data Modeling Methodology](/uploads/mongodb/methodology.png "MongoDB Data Modelling Methodology")

First, look at the workload which includes:
1. Size of data
2. Number of reads and writes
3. What types of reads, and what types of writes, especially temporal

Second, identify the relationships and properties:
1. Identify
2. Quantity
3. Qualify - one to many, zero to many, one to one, capped, reuse, duplicate
4. Embed or link (cross-ref)

Apply patterns; there are a number of recipes can that be utilised for common _data_ situations.

Repeat. Review. Repeat. Review. Repeat....

### Simplicity vs Performance
Model for simplicity or for performance. Iterative modelling on simplicity and then performance back to simplicity and then again for performance.

Simplicity would be few collections with embedded documents, supporting the most frequent queries. Atypical of small teams and small projects.

Performance requires analysis and considerations of #reads/#writes, latency, read/write concerns, sharding and workload isolation. Atypical of large projects and large teams, with data and knowledge experts.

# Identitfying Workload
![IoT Example Topology](/uploads/mongodb/iot-topology.png "IoT Example Topology")


[M320 IOT Workload Analysis](/uploads/M320-workload-IOT.pdf "M320 IOT Workload Analysis")


